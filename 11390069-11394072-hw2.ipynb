{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Information Retrieval I #\n",
    "## Assignment 2: retrieval models [100 points + 10 bonus points] ##\n",
    "**TA**: Christophe Van Gysel (cvangysel@uva.nl; C3.258B, Science Park 904)\n",
    "\n",
    "**Secondary TAs**: Harrie Oosterhuis, Nikos Voskarides"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment you will get familiar with basic information retrieval concepts. You will implement and evaluate different information retrieval ranking models and evaluate their performance.\n",
    "\n",
    "We provide you with a VirtualBox image that comes pre-loaded with an index and a Python installation. To query the index, you'll use a Python package ([pyndri](https://github.com/cvangysel/pyndri)) that allows easy access to the underlying document statistics.\n",
    "\n",
    "For evaluation you'll use the [TREC Eval](https://github.com/usnistgov/trec_eval) utility, provided by the National Institute of Standards and Technology of the United States. TREC Eval is the de facto standard way to compute Information Retrieval measures and is frequently referenced in scientific papers.\n",
    "\n",
    "This is a **groups-of-two assignment**, the deadline is **23:59 - 25 January, 2017**. Code quality, informative comments and convincing analysis of the results will be considered when grading. Submission should be done through blackboard, questions can be asked on the course [Piazza](https://piazza.com/class/ixoz63p156g1ts).\n",
    "\n",
    "### Technicalities (must-read!) ###\n",
    "This assignment comes pre-loaded on a VirtualBox running Ubuntu. We have configured the indexing software and Python environment such that it works out of the box. You are allowed to extract the files from the VirtualBox and set-up your own non-virtualized environment. However, in this case you are on your own w.r.t. software support.\n",
    "\n",
    "The assignment directory is organized as follows:\n",
    "   * `./assignment.ipynb` (this file): the description of the assignment.\n",
    "   * `./index/`: the index we prepared for you.\n",
    "   * `./ap_88_90/`: directory with ground-truth and evaluation sets:\n",
    "      * `qrel_test`: test query relevance collection (**test set**).\n",
    "      * `qrel_validation`: validation query relevance collection (**validation set**).\n",
    "      * `topics_title`: semicolon-separated file with query identifiers and terms.\n",
    "      \n",
    "`Python + Jupyter`, `Indri`, `Gensim` and `Pyndri` come pre-installed (see `$HOME/.local`). TREC Eval can be found in `$HOME/Downloads/trec_eval.9.0`. The password of the `student` account on the VirtualBox is `datascience`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TREC Eval primer ###\n",
    "The TREC Eval utility can be downloaded and compiled as follows:\n",
    "\n",
    "    git clone https://github.com/usnistgov/trec_eval.git\n",
    "    cd trec_eval\n",
    "    make\n",
    "\n",
    "TREC Eval computes evaluation scores given two files: ground-truth information regarding relevant documents, named *query relevance* or *qrel*, and a ranking of documents for a set of queries, referred to as a *run*. The *qrel* will be supplied by us and should not be changed. For every retrieval model (or combinations thereof) you will generate a run of the top-1000 documents for every query. The format of the *run* file is as follows:\n",
    "\n",
    "    $query_identifier Q0 $document_identifier $rank_of_document_for_query $query_document_similarity $run_identifier\n",
    "    \n",
    "where\n",
    "   * `$query_identifier` is the unique identifier corresponding to a query (usually this follows a sequential numbering).\n",
    "   * `Q0` is a legacy field that you can ignore.\n",
    "   * `$document_identifier` corresponds to the unique identifier of a document (e.g., APXXXXXXX where AP denotes the collection and the Xs correspond to a unique numerical identifier).\n",
    "   * `$rank_of_document_for_query` denotes the rank of the document for the particular query. This field is ignored by TREC Eval and is only maintained for legacy support. The ranks are computed by TREC Eval itself using the `$query_document_similarity` field (see next). However, it remains good practice to correctly compute this field.\n",
    "   * `$query_document_similarity` is a score indicating the similarity between query and document where a higher score denotes greater similarity.\n",
    "   * `$run_identifier` is an identifier of the run. This field is for your own convenience and has no purpose beyond bookkeeping.\n",
    "   \n",
    "For example, say we have two queries: `Q1` and `Q2` and we rank three documents (`DOC1`, `DOC2`, `DOC3`). For query `Q1`, we find the following similarity scores `score(Q1, DOC1) = 1.0`, `score(Q1, DOC2) = 0.5`, `score(Q1, DOC3) = 0.75`; and for `Q2`: `score(Q2, DOC1) = -0.1`, `score(Q2, DOC2) = 1.25`, `score(Q1, DOC3) = 0.0`. We can generate run using the following snippet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q2 Q0 DOC2 1 1.25 example\n",
      "Q2 Q0 DOC3 2 0.0 example\n",
      "Q2 Q0 DOC1 3 -0.1 example\n",
      "Q1 Q0 DOC1 1 1.0 example\n",
      "Q1 Q0 DOC3 2 0.75 example\n",
      "Q1 Q0 DOC2 3 0.5 example\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import sys\n",
    "\n",
    "def write_run(model_name, data, out_f,\n",
    "              max_objects_per_query=sys.maxsize,\n",
    "              skip_sorting=False):\n",
    "    \"\"\"\n",
    "    Write a run to an output file.\n",
    "    Parameters:\n",
    "        - model_name: identifier of run.\n",
    "        - data: dictionary mapping topic_id to object_assesments;\n",
    "            object_assesments is an iterable (list or tuple) of\n",
    "            (relevance, object_id) pairs.\n",
    "            The object_assesments iterable is sorted by decreasing order.\n",
    "        - out_f: output file stream.\n",
    "        - max_objects_per_query: cut-off for number of objects per query.\n",
    "    \"\"\"\n",
    "    for subject_id, object_assesments in data.items():\n",
    "        if not object_assesments:\n",
    "            logging.warning('Received empty ranking for %s; ignoring.',\n",
    "                            subject_id)\n",
    "\n",
    "            continue\n",
    "\n",
    "        # Probe types, to make sure everything goes alright.\n",
    "        # assert isinstance(object_assesments[0][0], float) or \\\n",
    "        #     isinstance(object_assesments[0][0], np.float32)\n",
    "        assert isinstance(object_assesments[0][1], str) or \\\n",
    "            isinstance(object_assesments[0][1], bytes)\n",
    "\n",
    "        if not skip_sorting:\n",
    "            object_assesments = sorted(object_assesments, reverse=True)\n",
    "\n",
    "        if max_objects_per_query < sys.maxsize:\n",
    "            object_assesments = object_assesments[:max_objects_per_query]\n",
    "\n",
    "        if isinstance(subject_id, bytes):\n",
    "            subject_id = subject_id.decode('utf8')\n",
    "\n",
    "        for rank, (relevance, object_id) in enumerate(object_assesments):\n",
    "            if isinstance(object_id, bytes):\n",
    "                object_id = object_id.decode('utf8')\n",
    "\n",
    "            out_f.write(\n",
    "                '{subject} Q0 {object} {rank} {relevance} '\n",
    "                '{model_name}\\n'.format(\n",
    "                    subject=subject_id,\n",
    "                    object=object_id,\n",
    "                    rank=rank + 1,\n",
    "                    relevance=relevance,\n",
    "                    model_name=model_name))\n",
    "            \n",
    "# The following writes the run to standard output.\n",
    "# In your code, you should write the runs to local\n",
    "# storage in order to pass them to trec_eval.\n",
    "write_run(\n",
    "    model_name='example',\n",
    "    data={\n",
    "        'Q1': ((1.0, 'DOC1'), (0.5, 'DOC2'), (0.75, 'DOC3')),\n",
    "        'Q2': ((-0.1, 'DOC1'), (1.25, 'DOC2'), (0.0, 'DOC3')),\n",
    "    },\n",
    "    out_f=sys.stdout,\n",
    "    max_objects_per_query=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, imagine that we know that `DOC1` is relevant and `DOC3` is non-relevant for `Q1`. In addition, for `Q2` we only know of the relevance of `DOC3`. The query relevance file looks like:\n",
    "\n",
    "    Q1 0 DOC1 1\n",
    "    Q1 0 DOC3 0\n",
    "    Q2 0 DOC3 1\n",
    "    \n",
    "We store the run and qrel in files `example.run` and `example.qrel` respectively on disk. We can now use TREC Eval to compute evaluation measures. In this example, we're only interested in Mean Average Precision and we'll only show this below for brevity. However, TREC Eval outputs much more information such as NDCG, recall, precision, etc.\n",
    "\n",
    "    $ trec_eval -m all_trec -q example.qrel example.run | grep -E \"^map\\s\"\n",
    "    > map                   \tQ1\t1.0000\n",
    "    > map                   \tQ2\t0.5000\n",
    "    > map                   \tall\t0.7500\n",
    "    \n",
    "Now that we've discussed the output format of rankings and how you can compute evaluation measures from these rankings, we'll now proceed with an overview of the indexing framework you'll use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pyndri primer ###\n",
    "For this assignment you will use [Pyndri](https://github.com/cvangysel/pyndri) [[1](https://arxiv.org/abs/1701.00749)], a python interface for [Indri](https://www.lemurproject.org/indri.php). We have indexed the document collection and you can query the index using Pyndri. We will start by giving you some examples of what Pyndri can do:\n",
    "\n",
    "First we read the document collection index with Pyndri:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pyndri\n",
    "\n",
    "index = pyndri.Index('index/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loaded index can be used to access a collection of documents in an easy manner. We'll give you some examples to get some idea of what it can do, it is up to you to figure out how to use it for the remainder of the assignment.\n",
    "\n",
    "First let's look at the number of documents, since Pyndri indexes the documents using incremental identifiers we can simply take the lowest index and the maximum document and consider the difference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 164597 documents in this collection.\n"
     ]
    }
   ],
   "source": [
    "print(\"There are %d documents in this collection.\" % (index.maximum_document() - index.document_base()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take the first document out of the collection and take a look at it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('AP890425-0001', (1360, 192, 363, 0, 880, 0, 200, 0, 894, 412, 92160, 3, 192, 0, 363, 34, 1441, 0, 174134, 0, 200, 0, 894, 412, 2652, 0, 810, 107, 49, 4903, 420, 0, 1, 48, 35, 489, 0, 35, 687, 192, 243, 0, 249311, 1877, 0, 1651, 1174, 0, 2701, 117, 412, 0, 810, 391, 245233, 1225, 5838, 16, 0, 233156, 3496, 0, 393, 17, 0, 2435, 4819, 930, 0, 0, 200, 0, 894, 0, 22, 398, 145, 0, 3, 271, 115, 0, 1176, 2777, 292, 0, 725, 192, 0, 0, 50046, 0, 1901, 1130, 0, 192, 0, 408, 0, 243779, 0, 0, 553, 192, 0, 363, 0, 3747, 0, 0, 0, 0, 1176, 0, 1239, 0, 0, 1115, 17, 0, 0, 585, 192, 1963, 0, 0, 412, 54356, 0, 773, 0, 0, 0, 192, 0, 0, 1130, 0, 363, 0, 545, 192, 0, 1174, 1901, 1130, 0, 4, 398, 145, 39, 0, 577, 0, 355, 0, 491, 0, 6025, 0, 0, 193156, 88, 34, 437, 0, 0, 1852, 0, 828, 0, 1588, 0, 0, 0, 2615, 0, 0, 107, 49, 420, 0, 0, 190, 7, 714, 2701, 0, 237, 192, 157, 0, 412, 34, 437, 0, 0, 200, 6025, 26, 0, 0, 0, 0, 363, 0, 22, 398, 145, 0, 200, 638, 126222, 6018, 0, 880, 0, 0, 161, 0, 0, 319, 894, 2701, 0, 0, 0, 301, 1200, 0, 363, 251, 430, 0, 207, 0, 76143, 1773, 0, 243779, 0, 0, 72030, 0, 55, 4903, 420, 0, 2701, 1496, 420, 0, 25480, 0, 420, 0, 0, 200, 0, 392, 2949, 0, 1738, 0, 61, 0, 71, 79, 0, 200, 903, 0, 188, 53, 6, 0, 476, 2, 0, 2028, 97, 334, 0, 0, 200, 178, 0, 0, 107, 49, 0, 214, 0, 0, 0, 114, 3866, 1505, 195, 79893, 574, 0, 198, 2160, 0, 192, 0, 420, 0, 384, 0, 2701, 0, 114, 6025, 1549, 74627, 0, 238, 0, 0, 0, 3729, 0, 192, 0, 79893, 0, 0, 729, 3141, 129, 0, 192, 196764, 39, 0, 0, 714, 63, 0, 55, 420, 3356, 0, 0, 117, 412, 0, 0, 79758, 0, 1901, 1130, 4067, 2133, 0, 0, 875, 72, 0, 0, 336, 2789, 0, 0, 25, 920, 121, 104, 0, 3162, 0, 0, 420, 0, 2178, 0, 0, 386, 192545, 159306, 0, 0, 0, 1914, 0, 200, 0, 1794, 0, 2654, 0, 0, 25480, 420, 0, 2795, 0, 0, 229690, 0, 32559, 0, 0, 392, 253919, 0, 0, 0, 0, 379, 0, 0, 114, 0, 553, 10, 0, 1128, 0, 23610, 248, 151, 0, 418, 0, 651, 0, 36, 0, 0, 645, 0, 0, 513, 0, 0, 25480, 420, 34, 0, 0, 0, 15, 0, 3348, 0, 3496, 0, 35, 687, 0, 1, 48, 0, 0, 2803, 0, 0, 714, 1274, 0, 114, 62, 1006, 70268, 1200, 2357, 0, 497, 0, 497, 125, 0, 913, 4647, 3985, 0, 0, 3370, 245233, 0, 0, 687, 0, 4, 1288, 0, 0, 0, 0, 715, 0, 0, 687, 583, 0, 0, 1627, 0, 0, 11, 357, 1359, 0, 849, 0, 0, 1518, 462, 245233, 0, 0, 0, 0, 0, 0, 171, 70268, 0)) lol\n"
     ]
    }
   ],
   "source": [
    "example_document = index.document(index.document_base())\n",
    "print(example_document, 'lol')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see a document consists of two things, a string representing the external document identifier and an integer list representing the identifiers of words that make up the document. Pyndri uses integer representations for words or terms, thus a token_id is an integer that represents a word whereas the token is the actual text of the word/term. Every id has a unique token and vice versa with the exception of stop words: words so common that there are uninformative, all of these receive the zero id.\n",
    "\n",
    "To see what some ids and their matching tokens mean we take a look at the dictionary of the index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 'new'), (2, 'percent'), (3, 'two'), (4, '1'), (5, 'people'), (6, 'million'), (7, '000'), (8, 'government'), (9, 'president'), (10, 'years'), (11, 'state'), (12, '2'), (13, 'states'), (14, 'three'), (15, 'time')]\n"
     ]
    }
   ],
   "source": [
    "token2id, id2token, _ = index.get_dictionary()\n",
    "print(list(id2token.items())[:15])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this dictionary we can see the tokens for the (non-stop) words in our example document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['52', 'students', 'arrested', 'takeover', 'university', 'massachusetts', 'building', 'fifty', 'two', 'students', 'arrested', 'tuesday', 'evening', 'occupying', 'university', 'massachusetts', 'building', 'overnight', 'protest', 'defense', 'department', 'funded', 'research', 'new', 'york', 'city', 'thousands', 'city', 'college', 'students', 'got', 'unscheduled', 'holiday', 'demonstrators', 'occupied', 'campus', 'administration', 'building', 'protest', 'possible', 'tuition', 'increases', 'prompting', 'officials', 'suspend', 'classes', '60', 'police', 'riot', 'gear', 'arrived', 'university', 'massachusetts', '5', 'p', 'm', 'two', 'hours', 'later', 'bus', 'drove', 'away', '29', 'students', 'camped', 'memorial', 'hall', 'students', 'charged', 'trespassing', '23', 'students', 'arrested', 'lying', 'bus', 'prevent', 'leaving', 'police', '300', 'students', 'stood', 'building', 'chanting', 'looking', 'students', 'hall', 'arrested', '35', 'students', 'occupied', 'memorial', 'hall', '1', 'p', 'm', 'monday', 'declined', 'offer', 'meet', 'administrators', 'provosts', 'office', 'tuesday', 'morning', 'presented', 'list', 'demands', 'halt', 'defense', 'department', 'research', '25', '000', 'student', 'campus', '40', 'students', 'left', 'building', 'tuesday', 'morning', 'university', 'administrators', 'told', 'arrested', '5', 'p', 'm', 'university', 'spokeswoman', 'jeanne', 'hopkins', 'takeover', 'second', 'western', 'massachusetts', 'campus', 'seven', 'protesters', 'arrested', 'april', '19', 'charges', 'disorderly', 'conduct', 'trespassing', 'demonstrating', 'military', 'funded', 'research', 'campus', 'particularly', 'research', 'anthrax', 'research', 'university', 'non', 'classified', 'researchers', 'make', 'work', 'public', 'university', 'rules', '11', '6', 'million', '22', 'percent', 'grant', 'money', 'received', 'university', 'came', 'defense', 'department', '1988', 'school', 'chancellor', 'joseph', 'd', 'duffey', 'issued', 'statement', 'telling', 'students', 'research', 'continue', 'campus', 'school', 'administrators', 'decide', 'differently', 'policy', 'negotiated', 'students', 'duffey', 'latest', 'occupation', 'began', 'students', 'rallying', 'monday', 'student', 'union', 'military', 'research', 'marched', 'administration', 'building', 'ducked', 'memorial', 'hall', 'en', 'route', 'followed', 'members', 'local', 'chapter', 'american', 'friends', 'service', 'committee', 'contended', 'research', 'dangerous', 'town', 'promotes', 'militarism', 'banned', 'university', 'argued', 'purpose', 'anthrax', 'research', 'peaceful', 'strain', 'bacteria', 'non', 'virulent', 'study', 'school', '23', 'years', 'incident', 'amherst', 'health', 'board', 'scheduled', 'hearing', 'wednesday', 'question', 'safety', 'anthrax', 'research', 'tuesday', 'time', '1969', 'classes', 'city', 'college', 'new', 'york', 'canceled', 'student', 'protests', 'school', 'spokesman', 'charles', 'deciccio', 'protesters', 'demanding', 'face', 'face', 'meeting', 'gov', 'mario', 'cuomo', 'feared', 'tuition', 'college', '1', '250', 'increased', 'college', 'staff', 'reduced', 'state', 'budget', 'cuts', 'governor', 'immediate', 'comment', 'tuition', 'set', 'deciccio']\n"
     ]
    }
   ],
   "source": [
    "print([id2token[word_id] for word_id in example_document[1] if word_id > 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reverse can also be done, say we want to look for news about the \"University of Massachusetts\", the tokens of that query can be converted to ids using the reverse dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query by tokens: ['university', '', 'massachusetts']\n",
      "Query by ids with stopwords: [200, 0, 894]\n",
      "Query by ids without stopwords: [200, 894]\n"
     ]
    }
   ],
   "source": [
    "query_tokens = index.tokenize(\"University of Massachusetts\")\n",
    "print(\"Query by tokens:\", query_tokens)\n",
    "query_id_tokens = [token2id.get(query_token,0) for query_token in query_tokens]\n",
    "print(\"Query by ids with stopwords:\", query_id_tokens)\n",
    "query_id_tokens = [word_id for word_id in query_id_tokens if word_id > 0]\n",
    "print(\"Query by ids without stopwords:\", query_id_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naturally we can now match the document and query in the id space, let's see how often a word from the query occurs in our example document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document AP890425-0001 has 13 word matches with query: \"university  massachusetts\".\n",
      "Document AP890425-0001 and query \"university  massachusetts\" have a 2.5% overlap.\n"
     ]
    }
   ],
   "source": [
    "matching_words = sum([True for word_id in example_document[1] if word_id in query_id_tokens])\n",
    "print(\"Document %s has %d word matches with query: \\\"%s\\\".\" % (example_document[0], matching_words, ' '.join(query_tokens)))\n",
    "print(\"Document %s and query \\\"%s\\\" have a %.01f%% overlap.\" % (example_document[0], ' '.join(query_tokens),matching_words/float(len(example_document[1]))*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While this is certainly not everything Pyndri can do, it should give you an idea of how to use it. Please take a look at the [examples](https://github.com/cvangysel/pyndri) as it will help you a lot with this assignment.\n",
    "\n",
    "**CAUTION**: Avoid printing out the whole index in this Notebook as it will generate a lot of output and is likely to corrupt the Notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing the query file\n",
    "You can parse the query file (`ap_88_89/topics_title`) using the following snippet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('51', 'Airbus Subsidies'), ('52', 'South African Sanctions'), ('53', 'Leveraged Buyouts'), ('54', 'Satellite Launch Contracts'), ('55', 'Insider Trading'), ('56', 'Prime (Lending) Rate Moves, Predictions'), ('57', 'MCI'), ('58', 'Rail Strikes'), ('59', 'Weather Related Fatalities'), ('60', 'Merit-Pay vs. Seniority'), ('61', 'Israeli Role in Iran-Contra Affair'), ('62', \"Military Coups D'etat\"), ('63', 'Machine Translation'), ('64', 'Hostage-Taking'), ('65', 'Information Retrieval Systems'), ('66', 'Natural Language Processing'), ('67', 'Politically Motivated Civil Disturbances'), ('68', 'Health Hazards from Fine-Diameter Fibers'), ('69', 'Attempts to Revive the SALT II Treaty'), ('70', 'Surrogate Motherhood'), ('71', 'Border Incursions'), ('72', 'Demographic Shifts in the U.S.'), ('73', 'Demographic Shifts across National Boundaries'), ('74', 'Conflicting Policy'), ('75', 'Automation'), ('76', 'U.S. Constitution - Original Intent'), ('77', 'Poaching'), ('78', 'Greenpeace'), ('79', 'FRG Political Party Positions'), ('80', '1988 Presidential Candidates Platforms'), ('81', 'Financial crunch for televangelists in the wake of the PTL scandal'), ('82', 'Genetic Engineering'), ('83', 'Measures to Protect the Atmosphere'), ('84', 'Alternative/renewable Energy Plant & Equipment Installation'), ('85', 'Official Corruption'), ('86', 'Bank Failures'), ('87', 'Criminal Actions Against Officers of Failed Financial Institutions'), ('88', 'Crude Oil Price Trends'), ('89', '\"Downstream\" Investments by OPEC Member States'), ('90', 'Data on Proven Reserves of Oil & Natural Gas Producers'), ('91', 'U.S. Army Acquisition of Advanced Weapons Systems'), ('92', 'International Military Equipment Sales'), ('93', 'What Backing Does the National Rifle Association Have?'), ('94', 'Computer-aided Crime'), ('95', 'Computer-aided Crime Detection'), ('96', 'Computer-Aided Medical Diagnosis'), ('97', 'Fiber Optics Applications'), ('98', 'Fiber Optics Equipment Manufacturers'), ('99', 'Iran-Contra Affair'), ('100', 'Controlling the Transfer of High Technology'), ('101', 'Design of the \"Star Wars\" Anti-missile Defense System'), ('102', \"Laser Research Applicable to the U.S.'s Strategic Defense Initiative\"), ('103', 'Welfare Reform'), ('104', 'Catastrophic Health Insurance'), ('105', '\"Black Monday\"'), ('106', 'U.S. Control of Insider Trading'), ('107', 'Japanese Regulation of Insider Trading'), ('108', 'Japanese Protectionist Measures'), ('109', 'Find Innovative Companies'), ('110', 'Black Resistance Against the South African Government'), ('111', 'Nuclear Proliferation'), ('112', 'Funding Biotechnology'), ('113', 'New Space Satellite Applications'), ('114', 'Non-commercial Satellite Launches'), ('115', 'Impact of the 1986 Immigration Law'), ('116', 'Generic Drug Substitutions'), ('117', 'Capacity of the U.S. Cellular Telephone Network'), ('118', 'International Terrorists'), ('119', 'Actions Against International Terrorists'), ('120', 'Economic Impact of International Terrorism'), ('121', 'Death from Cancer'), ('122', 'RDT&E of New Cancer Fighting Drugs'), ('123', 'Research into & Control of Carcinogens'), ('124', 'Alternatives to Traditional Cancer Therapies'), ('125', 'Anti-smoking Actions by Government'), ('126', 'Medical Ethics and Modern Technology'), ('127', 'U.S.-U.S.S.R. Arms Control Agreements'), ('128', 'Privatization of State Assets'), ('129', 'Soviet Spying on the U.S.'), ('130', 'Jewish Emigration and U.S.-USSR Relations'), ('131', 'McDonnell Douglas Contracts for Military Aircraft'), ('132', '\"Stealth\" Aircraft'), ('133', 'Hubble Space Telescope'), ('134', 'The Human Genome Project'), ('135', 'Possible Contributions of Gene Mapping to Medicine'), ('136', 'Diversification by Pacific Telesis'), ('137', 'Expansion in the U.S. Theme Park Industry'), ('138', 'Iranian Support for Lebanese Hostage-takers'), ('139', \"Iran's Islamic Revolution - Domestic and Foreign Social Consequences\"), ('140', 'Political Impact of Islamic Fundamentalism'), ('141', \"Japan's Handling of its Trade Surplus with the U.S.\"), ('142', 'Impact of Government Regulated Grain Farming on International Relations'), ('143', 'Why Protect U.S. Farmers?'), ('144', 'Management Problems at the United Nations'), ('145', 'Influence of the \"Pro-Israel Lobby\"'), ('146', 'Negotiating an End to the Nicaraguan Civil War'), ('147', 'Productivity Trends in the U.S. Economy'), ('148', 'Conflict in the Horn of Africa'), ('149', 'Industrial Espionage'), ('150', 'U.S. Political Campaign Financing'), ('151', 'Coping with overcrowded prisons'), ('152', 'Accusations of Cheating by Contractors on U.S. Defense Projects'), ('153', 'Insurance Coverage which pays for Long Term Care'), ('154', 'Oil Spills'), ('155', 'Right Wing Christian Fundamentalism in U.S.'), ('156', 'Efforts to enact Gun Control Legislation'), ('157', 'Causes and treatments of multiple sclerosis (MS)'), ('158', 'Term limitations for members of the U.S. Congress'), ('159', 'Electric Car Development'), ('160', 'Vitamins - The Cure for or Cause of Human Ailments'), ('161', 'Acid Rain'), ('162', 'Automobile Recalls'), ('163', 'Vietnam Veterans and Agent Orange'), ('164', 'Generic Drugs - Illegal Activities by Manufacturers'), ('165', 'Tobacco company advertising and the young'), ('166', 'Standardized testing and cultural bias'), ('167', 'Regulation of the showing of violence and explicit sex in motion picture theaters, on television, and on video cassettes.'), ('168', 'Financing AMTRAK'), ('169', 'Cost of Garbage/Trash Removal'), ('170', 'The Consequences of Implantation of Silicone Gel Breast Devices'), ('171', \"Use of Mutual Funds in an Individual's Retirement Strategy\"), ('172', 'The Effectiveness of Medical Products and Related Programs Utilized in the Cessation of Smoking.'), ('173', 'Smoking Bans'), ('174', 'Hazardous Waste Cleanup'), ('175', 'NRA Prevention of Gun Control Legislation'), ('176', 'Real-life private investigators'), ('177', 'English as the Official Language in U.S.'), ('178', 'Dog Maulings'), ('179', 'U. S. Restaurants in Foreign Lands'), ('180', 'Ineffectiveness of U.S. Embargoes/Sanctions'), ('181', 'Abuse of the Elderly by Family Members, and Medical and Nonmedical Personnel, and Initiatives Being Taken to Minimize This Mistreatment'), ('182', 'Commercial Overfishing Creates Food Fish Deficit'), ('183', 'Asbestos Related Lawsuits'), ('184', 'Corporate Pension Plans/Funds'), ('185', 'Reform of the U.S. Welfare System'), ('186', 'Difference of Learning Levels Among Inner City and More Suburban School Students'), ('187', 'Signs of the Demise of Independent Publishing'), ('188', 'Beachfront Erosion'), ('189', 'Real Motives for Murder'), ('190', 'Instances of Fraud Involving the Use of a Computer'), ('191', 'Efforts to Improve U.S. Schooling'), ('192', 'Oil Spill Cleanup'), ('193', 'Toys R Dangerous'), ('194', 'The Amount of Money Earned by Writers'), ('195', 'Stock Market Perturbations Attributable to Computer Initiated Trading'), ('196', 'School Choice Voucher System and its effects upon the entire U.S. educational program'), ('197', 'Reform of the jurisprudence system to stop juries from granting unreasonable monetary awards'), ('198', 'Gene Therapy and Its Benefits to Humankind'), ('199', 'Legality of Medically Assisted Suicides'), ('200', 'Impact of foreign textile imports on U.S. textile industry')])\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "import io\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "def parse_topics(file_or_files,\n",
    "                 max_topics=sys.maxsize, delimiter=';'):\n",
    "    assert max_topics >= 0 or max_topics is None\n",
    "\n",
    "    topics = collections.OrderedDict()\n",
    "\n",
    "    if not isinstance(file_or_files, list) and \\\n",
    "            not isinstance(file_or_files, tuple):\n",
    "        if hasattr(file_or_files, '__iter__'):\n",
    "            file_or_files = list(file_or_files)\n",
    "        else:\n",
    "            file_or_files = [file_or_files]\n",
    "\n",
    "    for f in file_or_files:\n",
    "        assert isinstance(f, io.IOBase)\n",
    "\n",
    "        for line in f:\n",
    "            assert(isinstance(line, str))\n",
    "\n",
    "            line = line.strip()\n",
    "\n",
    "            if not line:\n",
    "                continue\n",
    "\n",
    "            topic_id, terms = line.split(delimiter, 1)\n",
    "\n",
    "            if topic_id in topics and (topics[topic_id] != terms):\n",
    "                    logging.error('Duplicate topic \"%s\" (%s vs. %s).',\n",
    "                                  topic_id,\n",
    "                                  topics[topic_id],\n",
    "                                  terms)\n",
    "\n",
    "            topics[topic_id] = terms\n",
    "\n",
    "            if max_topics > 0 and len(topics) >= max_topics:\n",
    "                break\n",
    "\n",
    "    return topics\n",
    "\n",
    "with open('./ap_88_89/topics_title', 'r') as f_topics:\n",
    "    print(parse_topics([f_topics]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: Implement and compare lexical IR methods [45 points] ### \n",
    "\n",
    "In this task you will implement a number of lexical methods for IR using the **Pyndri** framework. Then you will evaluate these methods on the dataset we have provided using **TREC Eval**.\n",
    "\n",
    "Use the **Pyndri** framework to get statistics of the documents (term frequency, document frequency, collection frequency; **you are not allowed to use the query functionality of Pyndri**) and implement the following scoring methods in **Python**:\n",
    "\n",
    "- [TF-IDF](http://nlp.stanford.edu/IR-book/html/htmledition/tf-idf-weighting-1.html). **[5 points]**\n",
    "- [BM25](http://nlp.stanford.edu/IR-book/html/htmledition/okapi-bm25-a-non-binary-model-1.html) with k1=1.2 and b=0.75. **[5 points]**\n",
    "- Language models ([survey](https://drive.google.com/file/d/0B-zklbckv9CHc0c3b245UW90NE0/view))\n",
    "    - Jelinek-Mercer (explore different values of 𝛌 in the range [0.1, 0.2, ..., 0.9]). **[5 points]**\n",
    "    - Dirichlet Prior (explore different values of 𝛍 [500, 1000, ..., 2000]). **[5 points]**\n",
    "    - Absolute discounting (explore different values of 𝛅 in the range [0.1, 0.2, ..., 0.9]). **[5 points]**\n",
    "    - [Positional Language Models](http://sifaka.cs.uiuc.edu/~ylv2/pub/sigir09-plm.pdf) define a language model for each position of a document, and score a document based on the scores of its PLMs. The PLM is estimated based on propagated counts of words within a document through a proximity-based density function, which both captures proximity heuristics and achieves an effect of “soft” passage retrieval. Implement the PLM, all five kernels, but only the Best position strategy to score documents. Use 𝛔 equal to 50, and Dirichlet smoothing with 𝛍 optimized on the validation set (decide how to optimize this value yourself and motivate your decision in the report). **[10 points]**\n",
    "    \n",
    "Implement the above methods and report evaluation measures (on the test set) using the hyper parameter values you optimized on the validation set (also report the values of the hyper parameters). Use TREC Eval to obtain the results and report on `NDCG@10`, Mean Average Precision (`MAP@1000`), `Precision@5` and `Recall@1000`.\n",
    "\n",
    "For the language models, create plots showing `NDCG@10` with varying values of the parameters. You can do this by chaining small scripts using shell scripting (preferred) or execute trec_eval using Python's `subprocess`.\n",
    "\n",
    "Compute significance of the results using a [two-tailed paired Student t-test](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.ttest_rel.html) **[10 points]**. Be wary of false rejection of the null hypothesis caused by the [multiple comparisons problem](https://en.wikipedia.org/wiki/Multiple_comparisons_problem). There are multiple ways to mitigate this problem and it is up to you to choose one.\n",
    "\n",
    "Analyse the results by identifying specific queries where different methods succeed or fail and discuss possible reasons that cause these differences.\n",
    "\n",
    "**NOTE**: Don’t forget to use log computations in your calculations to avoid underflows. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Use the **Pyndri** framework to get statistics of the documents (term frequency, document frequency, collection frequency; **you are not allowed to use the query functionality of Pyndri**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75056232\n",
      "0\n",
      "10000000\n",
      "20000000\n",
      "30000000\n",
      "40000000\n",
      "50000000\n",
      "60000000\n",
      "70000000\n",
      "461.63406987976697\n"
     ]
    }
   ],
   "source": [
    "import pyndri\n",
    "import numpy as np\n",
    "\n",
    "added_word = []\n",
    "query_set= set()\n",
    "#dictionary of document id to dictionnary of word index to word frequency{doc_id,{word_id, word_frequency}}\n",
    "doc_terms_frequency = {}\n",
    "nb_words_in_doc={}\n",
    "#number of unique words in document\n",
    "nb_distinct_words_in_doc = {}\n",
    "#number of document in which the word appears / total number of documents\n",
    "document_frequency={}\n",
    "total_document_nb=(index.maximum_document()-index.document_base())\n",
    "#total number of occurence of a term in the collection\n",
    "collection_frequency={}\n",
    "total_collection_words=0 \n",
    "\n",
    "#create the set of query words\n",
    "with open('./ap_88_89/topics_title', 'r') as f_topics:\n",
    "    for key, value in parse_topics([f_topics]).items():\n",
    "        for token in index.tokenize(value):\n",
    "            token_id= token2id.get(token,0)\n",
    "            if token_id>0:\n",
    "                query_set.add(token_id)\n",
    "                \n",
    "print(len(query_set)*(index.maximum_document()-index.document_base()))\n",
    "i=0\n",
    "for document_id in range(index.document_base(), index.maximum_document()):\n",
    "    doc = index.document(document_id)\n",
    "    for word_id in doc[1]:\n",
    "        if(i%10000000==0):\n",
    "            print(i)\n",
    "        if word_id in query_set:\n",
    "            #calculate document frequency \n",
    "            if word_id not in added_word:\n",
    "                if word_id not in document_frequency:\n",
    "                    document_frequency[word_id]=1\n",
    "                else:\n",
    "                    document_frequency[word_id]+=1\n",
    "                added_word.append(word_id)\n",
    "            #calculate terms frequency \n",
    "            if document_id not in doc_terms_frequency:\n",
    "                doc_terms_frequency[document_id]={}\n",
    "                doc_terms_frequency[document_id][word_id]=1\n",
    "            else:\n",
    "                if word_id not in  doc_terms_frequency[document_id]:\n",
    "                    doc_terms_frequency[document_id][word_id]=1\n",
    "                else:\n",
    "                    doc_terms_frequency[document_id][word_id]+=1\n",
    "            #calculate collection_frequency\n",
    "            if word_id not in  collection_frequency:\n",
    "                collection_frequency[word_id]=1\n",
    "            else: \n",
    "                collection_frequency[word_id]+=1\n",
    "        \n",
    "        i+=1\n",
    "    total_collection_words+=len(doc[1])\n",
    "    nb_words_in_doc[document_id]=len(doc[1])\n",
    "    nb_distinct_words_in_doc[document_id]=len(set(doc[1]))     \n",
    "    \n",
    "#reset added_word for memory reason: it is not used anymore \n",
    "added_word=[]\n",
    "\n",
    "average_doc_length = 0\n",
    "def calculate_avg_length():\n",
    "    average_doc_length = 0\n",
    "    total_doc_length = 0\n",
    "    for document_id in range(index.document_base(), index.maximum_document()):\n",
    "        doc = index.document(document_id)\n",
    "        total_doc_length += len(doc[1])\n",
    "    average_doc_length = total_doc_length / float(index.maximum_document() - index.document_base())\n",
    "    return average_doc_length\n",
    "average_doc_length = calculate_avg_length()\n",
    "print(average_doc_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.1)\n",
    "- [TF-IDF](http://nlp.stanford.edu/IR-book/html/htmledition/tf-idf-weighting-1.html). **[5 points]**\n",
    "- [BM25](http://nlp.stanford.edu/IR-book/html/htmledition/okapi-bm25-a-non-binary-model-1.html) with k1=1.2 and b=0.75. **[5 points]**\n",
    "- Language models ([survey](https://drive.google.com/file/d/0B-zklbckv9CHc0c3b245UW90NE0/view))\n",
    "    - Jelinek-Mercer (explore different values of 𝛌 in the range [0.1, 0.2, ..., 0.9]). **[5 points]**\n",
    "    - Dirichlet Prior (explore different values of 𝛍 [500, 1000, ..., 2000]). **[5 points]**\n",
    "    - Absolute discounting (explore different values of 𝛅 in the range [0.1, 0.2, ..., 0.9]). **[5 points]**\n",
    "    - [Positional Language Models](http://sifaka.cs.uiuc.edu/~ylv2/pub/sigir09-plm.pdf) define a language model for each position of a document, and score a document based on the scores of its PLMs. The PLM is estimated based on propagated counts of words within a document through a proximity-based density function, which both captures proximity heuristics and achieves an effect of “soft” passage retrieval. Implement the PLM, all five kernels, but only the Best position strategy to score documents. Use 𝛔 equal to 50, and Dirichlet smoothing with 𝛍 optimized on the validation set (decide how to optimize this value yourself and motivate your decision in the report). **[10 points]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Estimated time: 12sec \n",
    "#Preprocess some data in order to get the score faster \n",
    "#create the query dictionnary \n",
    "query_dict={}\n",
    "with open('./ap_88_89/topics_title', 'r') as f_topics:\n",
    "    for key, value in parse_topics([f_topics]).items():\n",
    "        query_dict[key]=[token2id.get(term,0) for term in index.tokenize(value) if(token2id.get(term,0) > 0)] \n",
    "\n",
    "#create the inverted index for words of the query to the document id they appear in\n",
    "term_to_doc= {}\n",
    "for document_id in range(index.document_base(), index.maximum_document()):\n",
    "    doc = index.document(document_id)\n",
    "    for term_id in doc[1]:\n",
    "        if term_id in query_set:\n",
    "            if term_id not in term_to_doc:\n",
    "                term_to_doc[term_id]=set()\n",
    "            term_to_doc[term_id].add(document_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# kernel function for PLM\n",
    "def kernel_PLM(gamma, i, j, pick):\n",
    "    if pick == \"gaussian\":\n",
    "        return np.exp( -1 * np.power((i-j),2) / float(np.power(gamma,2)) * 2)\n",
    "    elif pick == \"triangle\":\n",
    "        if np.absolute(i-j) < gamma:\n",
    "            return 1 - np.absolute(i-j)\n",
    "        else:\n",
    "            return 0\n",
    "    elif pick == \"hamming\":\n",
    "        if np.absolute(i-j) < gamma:\n",
    "            return 0.5 * (1 + np.cos(np.absolute(i-j) * np.pi / float(gamma)))\n",
    "        else:\n",
    "            return 0\n",
    "    elif pick == \"circle\":\n",
    "        if np.absolute(i-j) < gamma:\n",
    "            return np.sqrt(1 - np.power(np.absolute(i-j) / float(gamma) , 2))\n",
    "        else:\n",
    "            return 0\n",
    "    elif pick == \"passage\":\n",
    "        if np.absolute(i-j) < gamma:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.346573590288\n"
     ]
    }
   ],
   "source": [
    "#normalized, but still needs checking\n",
    "def TF_IDF(term_id,document):\n",
    "    tf = 0\n",
    "    #If a word has a non nul tf, his doc frequency is non nul so this value will be updated \n",
    "    #Otherwise, the tf is nul and this value will not matter \n",
    "    inv_doc_freq=0\n",
    "    if document in doc_terms_frequency:\n",
    "        if term_id in doc_terms_frequency[document]:\n",
    "            tf = doc_terms_frequency[document][term_id] / float(nb_words_in_doc[document])\n",
    "    if term_id in document_frequency:\n",
    "        inv_doc_freq=1.0/(document_frequency[term_id] / float(index.maximum_document() - index.document_base()))\n",
    "    return np.log(1+tf) * np.log(inv_doc_freq)\n",
    "\n",
    "def BM25(term_id,document,k1=1.5,b=0.75):\n",
    "    tf = 0\n",
    "    idf = 0\n",
    "    if document in doc_terms_frequency:\n",
    "        if term_id in doc_terms_frequency[document]:\n",
    "            tf= doc_terms_frequency[document][term_id] / float(nb_words_in_doc[document])\n",
    "    if term_id in document_frequency:\n",
    "        idf=1.0/(document_frequency[term_id] / float(index.maximum_document() - index.document_base()))\n",
    "    nom = ((k1 + 1) * tf) * idf\n",
    "    denom = k1 * ((1-b) + b * (nb_words_in_doc[document] / average_doc_length) + tf)\n",
    "    return nom / denom\n",
    "\n",
    "#return the probability to see a word given a document\n",
    "#LAMBDA TO BE TUNED \n",
    "def JelinekM (term_id,document,lamb_JM= 0.1):\n",
    "    tf_w_d = 0 \n",
    "    tf_w_c = 0 \n",
    "    if document in doc_terms_frequency:\n",
    "        if term_id in doc_terms_frequency[document]:\n",
    "            tf_w_d= doc_terms_frequency[document][term_id] / float(nb_words_in_doc[document]) \n",
    "    if term_id in collection_frequency:\n",
    "        tf_w_c=collection_frequency[term_id] / float(total_collection_words)\n",
    "    return lamb_JM* tf_w_d + (1-lamb_JM) * tf_w_c\n",
    "\n",
    "# mu to be tuned\n",
    "def dirichlet_prior(term_id,document,mu=500):\n",
    "    doc_length = float(nb_words_in_doc[document])\n",
    "    tf, df = 0, 0\n",
    "    if document in doc_terms_frequency:\n",
    "        if term_id in doc_terms_frequency[document]:\n",
    "            tf = doc_terms_frequency[document][term_id] / float(nb_words_in_doc[document])\n",
    "    if term_id in document_frequency:\n",
    "        df=document_frequency[term_id] / float(index.maximum_document() - index.document_base())\n",
    "    first = doc_length / float(doc_length + mu) * tf / float(doc_length)\n",
    "    second = mu / float(mu + doc_length) * (df / float(index.maximum_document() - index.document_base()))\n",
    "    return first + second\n",
    "\n",
    "# delta to be tuned\n",
    "def absolute_discount(term_id, document, delta=0.1):\n",
    "    doc_length = float(nb_words_in_doc[document])\n",
    "    unique_doc_length = float(nb_distinct_words_in_doc[document])\n",
    "    tf, df = 0, 0\n",
    "    if document in doc_terms_frequency:\n",
    "        if term_id in doc_terms_frequency[document]:\n",
    "            tf = doc_terms_frequency[document][term_id] / float(nb_words_in_doc[document])\n",
    "    if term_id in document_frequency:\n",
    "        df=document_frequency[term_id] / float(index.maximum_document() - index.document_base())\n",
    "    first = max(tf - delta,0) / doc_length\n",
    "    second = delta * unique_doc_length / doc_length * (df / float(index.maximum_document() - index.document_base()))\n",
    "    return first + second\n",
    "\n",
    "# do we treat p(w|Q) as zero if the word in doc does not appear in query?\n",
    "# p(w|Q) is the number of term appeared in a query divided by the number of query length\n",
    "# this is a query_id_list and doc_id function, not term_id and doc_id\n",
    "def PLM(query_list, document, kn=\"gaussian\", mu=500, gamma=50):\n",
    "    c = {}\n",
    "    sqd = -10000000\n",
    "    doc = index.document(document)\n",
    "    for i in range(len(doc[1])):\n",
    "        sqdi = 0\n",
    "        for query_word_id in set(query_list):\n",
    "            # calculate pwq\n",
    "            pwq = 0\n",
    "            # calculate the occurence of unique word in query\n",
    "            if query_word_id > 0:\n",
    "                pwq = query_list.count(query_word_id) / float(len(query_list))       \n",
    "            if(pwq == 0):\n",
    "                sqdi += 0\n",
    "            else:\n",
    "                # calculate pwDi\n",
    "                c[i] = {}\n",
    "                # calculate denom for pwDi\n",
    "                # for unique_word_2 in unique_word_id:\n",
    "                    # calculate c_hat, loop over index j times kernel function\n",
    "                    # c_temp = 0\n",
    "                for j in range(len(doc[1])):\n",
    "                    word_id = doc[1][j]\n",
    "                    if word_id not in c[i]:\n",
    "                        c[i][word_id] = 0\n",
    "                    # choose kernel : [gaussian, circle, hamming, passage, triangle]\n",
    "                    c[i][word_id] += 1 * kernel_PLM(gamma, i, j, kn)\n",
    "                #denominator for pwDi. \n",
    "                denom = sum(c[i].values())\n",
    "                # dirichlet smoothing\n",
    "                df = 0\n",
    "                if query_word_id in document_frequency:\n",
    "                    df=document_frequency[query_word_id] / float(index.maximum_document() - index.document_base())\n",
    "                pwC = df / float(index.maximum_document() - index.document_base())\n",
    "                # smoothed pwDi\n",
    "                pwDi = c[i][query_word_id] + (mu * pwC) / float(denom + mu)\n",
    "                # sqd\n",
    "                sqdi += pwq * np.log(pwq / pwDi) \n",
    "        sqdi = sqdi * -1\n",
    "        # get the maximum value of sqd, THIS IS BEST POSITION STRATEGY\n",
    "        temp = max(sqd, sqdi)\n",
    "        sqd = temp\n",
    "    return sqd\n",
    "\n",
    "query_test = \"1988 Presidential Candidates Platforms\"\n",
    "qt_list = [token2id.get(term,0) for term in query_test.split()]\n",
    "print(PLM(qt_list, index.document_base(), \"gaussian\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.2) Testing different values of 𝛌(Jelinek Mercer), 𝛍(Dirichlet prior), 𝛅(Absolute discount) and 𝛍(PLM) and optimizing on the validation set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 /72000\n",
      "10 /72000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-79-c5f70e8418d3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprog_PLM\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"/\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m24000\u001b[0m\u001b[0;34m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmu_PLM_range\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0mprog_PLM\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m             \u001b[0mval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mPLM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mterm_id_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdocument_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"gaussian\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmu_PLM_range\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0mscores_PLM\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mquery_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdocument\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-78-097942f2acfa>\u001b[0m in \u001b[0;36mPLM\u001b[0;34m(query_list, document, kn, mu, gamma)\u001b[0m\n\u001b[1;32m     93\u001b[0m                         \u001b[0mc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword_id\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m                     \u001b[0;31m# choose kernel : [gaussian, circle, hamming, passage, triangle]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m                     \u001b[0mc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword_id\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mkernel_PLM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m                 \u001b[0;31m#denominator for pwDi.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m                 \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Takes too much time to be run : approximated time is 16 000 minutes\n",
    "#DO NOT RUN: EXTREMELY LONG AND MEMORY CONSUMMING : compute the values for the run files (score) for PLM\n",
    "mu_PLM_range=[10,20,60]\n",
    "scores_PLM = []\n",
    "for i in range(len(mu_PLM_range)):\n",
    "    scores_PLM.append({})\n",
    "prog_PLM=0\n",
    "document_denom={}\n",
    "for query_id, term_id_list in query_dict.items():\n",
    "    for j in range(len (mu_PLM_range)):\n",
    "        scores_PLM[j][query_id] = []\n",
    "        document_set=set()\n",
    "        for term_id in term_id_list:\n",
    "                document_set= document_set | set(term_to_doc[term_id])\n",
    "        for document_id in document_set:\n",
    "            if prog_PLM%10==0:\n",
    "                print(prog_PLM,\"/\" + str(24000* len(mu_PLM_range)))\n",
    "            prog_PLM+=1\n",
    "            val=PLM(term_id_list,document_id,\"gaussian\",mu_PLM_range[j])\n",
    "            \n",
    "            scores_PLM[j][query_id].append((val,index.document(doc_id)[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Can't run: PLM is too slow \n",
    "#DO NOT RUN (write the run files given the previous scores)\n",
    "for j in range(0,len(scores_PLM)):\n",
    "    name=\"PLM_g_\"+str(mu_PLM_range[j])\n",
    "    write_run(model_name='tf-idf',data=scores[j],out_f=open('./ap_88_89/'+name+'.run', 'w'),max_objects_per_query=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#DO NOT RUN: EXTREMELY LONG AND MEMORY CONSUMMING : compute the values for the run files (score)\n",
    "#Tuning hyperparameters \n",
    "lambda_range= [0.1,0.4,0.7,0.9]\n",
    "mu_range= [500,1000,1500,2000]\n",
    "delta_range= [0.1,0.4,0.7,0.9]\n",
    "#Array of Dictionnary of term to dictionnary of doc_ID to score\n",
    "length = len(lambda_range)+len(mu_range)+len(delta_range)\n",
    "word_document_score = []\n",
    "for i in range(length):\n",
    "    word_document_score.append({})\n",
    "#Array of Dictionnary of query to tuple (score, document)\n",
    "scores = []\n",
    "for i in range(length):\n",
    "    scores.append({})\n",
    "    \n",
    "prog =0\n",
    "for term_id in query_set:\n",
    "    if term_id not in word_document_score[0]:\n",
    "        for j in range(0,len(scores)):\n",
    "            word_document_score[j][term_id] = {}\n",
    "    for document_id in term_to_doc[term_id]:\n",
    "        if prog%1000000==0:\n",
    "            print(prog,\"/2 600 000\")\n",
    "        prog+=1\n",
    "        #compute scores\n",
    "        for i in range(0,len(lambda_range)):\n",
    "            word_document_score[i][term_id][document_id]=JelinekM(term_id, document_id,lambda_range[i])\n",
    "        for i in range(0,len(mu_range)):\n",
    "            word_document_score[i+len(lambda_range)][term_id][document_id]=dirichlet_prior(term_id, document_id,mu_range[i])\n",
    "        for i in range(0,len(delta_range)):\n",
    "            word_document_score[i+len(lambda_range)+len(mu_range)][term_id][document_id]=absolute_discount(term_id, document_id,delta_range[i])\n",
    "\n",
    "prog=0\n",
    "            \n",
    "for query_id, term_id_list in query_dict.items():\n",
    "    for j in range(0,len(scores)):\n",
    "        scores[j][query_id] = []\n",
    "        temp_doc_to_score={}\n",
    "        for term_id in term_id_list:\n",
    "            for document_id in term_to_doc[term_id]:\n",
    "                if prog%1000000==0:\n",
    "                    print(prog,\"/\" + str(4100000 * length ))\n",
    "                prog+=1\n",
    "                if document_id not in temp_doc_to_score:\n",
    "                    temp_doc_to_score[document_id]=0\n",
    "                temp_doc_to_score[document_id]+=word_document_score[j][term_id][document_id]\n",
    "        for doc_id in temp_doc_to_score:\n",
    "            scores[j][query_id].append((temp_doc_to_score[doc_id], index.document(doc_id)[0]))\n",
    "        #reset variable to save memory\n",
    "        temp_doc_to_score={}\n",
    "#reset variable to save memory\n",
    "word_document_score= []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#DO NOT RUN (write the run files given the previous scores)\n",
    "for j in range(0,len(scores)):\n",
    "    name=\"\"\n",
    "    ind=0\n",
    "    if(j<len(lambda_range)):\n",
    "        ind=j\n",
    "        name=\"jelineM_\"+str(lambda_range[ind])\n",
    "    elif(len(lambda_range)<=j and j< (len(lambda_range)+len(mu_range))):\n",
    "        ind=j-len(lambda_range)\n",
    "        name=\"dirichlet_prior_\"+str(mu_range[ind])\n",
    "    else:\n",
    "        ind=j-len(lambda_range)-len(mu_range)\n",
    "        name=\"absolute_discount_\"+str(delta_range[ind])\n",
    "        # data={\n",
    "        # 'Q1': ((1.0, 'DOC1'), (0.5, 'DOC2'), (0.75, 'DOC3')),\n",
    "        # 'Q2': ((-0.1, 'DOC1'), (1.25, 'DOC2'), (0.0, 'DOC3')),\n",
    "    # }\n",
    "    write_run(model_name='tf-idf',data=scores[j],out_f=open('./ap_88_89/'+name+'.run', 'w'),max_objects_per_query=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Usefull function to display trec_eval measures: \n",
    "import os\n",
    "import random as rand \n",
    "from subprocess import Popen, PIPE, CalledProcessError\n",
    "\n",
    "def executeTrecEval(filename,validation):\n",
    "    qrel = \"validation\" if validation else \"test\"\n",
    "    cmd =[\"trec_eval\",\"-m\",\"all_trec\",\"-q\",\"ap_88_89/qrel_\"+qrel,\"ap_88_89/\"+filename+\".run\"]\n",
    "    p = Popen(cmd, stdout=PIPE, stderr=PIPE)\n",
    "    stdout, stderr = p.communicate()\n",
    "    return (stdout)\n",
    "\n",
    "def getData(tex,filt=None):\n",
    "    tex=tex.replace(\"\\\\t\",\" \")\n",
    "    tex=tex.replace(\"\\\\n\",\" \")\n",
    "    tex=tex.replace('b\"',\"\")\n",
    "    tex=tex.replace('\"',\"\")\n",
    "    tex_array=tex.split(\" \")\n",
    "    tex_array=[x for x in tex_array if (x!='')]\n",
    "    tex_array2=[]\n",
    "    for i in range(len(tex_array)):\n",
    "        if i%3 ==0:\n",
    "            tex_array2.append([tex_array[i]])\n",
    "        else:\n",
    "            tex_array2[i//3].append(tex_array[i])\n",
    "    good_index=[]\n",
    "    res=[]\n",
    "    if(filt!=None and len(filt)>0):\n",
    "        for j in range(len(filt)):\n",
    "            for k in range(len(tex_array2)):\n",
    "                if tex_array2[k][0]==filt[j]:\n",
    "                    good_index.append(k)\n",
    "        for l in range(len(good_index)):\n",
    "            res.append(tex_array2[good_index[l]])\n",
    "        return res\n",
    "    return tex_array2\n",
    "\n",
    "def getMean(array):\n",
    "    delete_index=[]\n",
    "    for k in range(len(array)):\n",
    "        if array[k][1]!=\"all\":\n",
    "            delete_index.append(k)\n",
    "    for l in range(len(delete_index)):\n",
    "        del array[delete_index[l]-l]\n",
    "    return array\n",
    "\n",
    "def whoWins2(meanArray1,meanArray2):\n",
    "    win1=0\n",
    "    win2=0\n",
    "    for k in range(len(meanArray1)):\n",
    "        if (float(meanArray1[k][2])>float(meanArray2[k][2])):\n",
    "            win1+=1\n",
    "        elif (float(meanArray1[k][2])<float(meanArray2[k][2])):\n",
    "            win2+=1\n",
    "    if win1>win2:\n",
    "        return 1\n",
    "    elif win1==win2:\n",
    "        #same score: return a random one \n",
    "        coin=rand.randint(0,1)\n",
    "        return 1 if(coin==0) else -1\n",
    "    else :\n",
    "        return -1\n",
    "def whoWins(list_mean_array):\n",
    "    winner_index=0\n",
    "    for i in range(1,len(list_mean_array)):\n",
    "        if(whoWins2(list_mean_array[i],list_mean_array[winner_index])==1):\n",
    "            winner_index=i\n",
    "    return winner_index\n",
    "           \n",
    "def dataToString(array,filt=None):\n",
    "    s=''\n",
    "    for i in range(len(array)):\n",
    "        elem=array[i]\n",
    "        s+=elem[0]+\" \"+elem[1]+\" \"+elem[2]+\"\\n\"\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best lambda for jelinek mercer 0.9\n",
      "Best mu for dirichlet_prior 2000\n",
      "Best delta for absolute_discount 0.4\n"
     ]
    }
   ],
   "source": [
    "#Use trec_eval to get the best parameter \n",
    "import os\n",
    "from subprocess import Popen, PIPE, CalledProcessError\n",
    "\n",
    "lambda_range= [0.1,0.4,0.7,0.9]\n",
    "list_mean_lambda=[]\n",
    "filename= \"jelineM_\"\n",
    "for i in range(len(lambda_range)):\n",
    "    stdout = executeTrecEval(filename+str(lambda_range[i]),True)\n",
    "    list_mean_lambda.append(getMean(getData(str(stdout),[\"P_1000\",\"recall_1000\",\"ndcg\"])))\n",
    "tuned_lambda= lambda_range[whoWins(list_mean_lambda)]\n",
    "print(\"Best lambda for jelinek mercer\",tuned_lambda)\n",
    "\n",
    "mu_range= [500,1000,1500,2000]\n",
    "list_mean_mu=[]\n",
    "filename= \"dirichlet_prior_\"\n",
    "for i in range(len(mu_range)):\n",
    "    stdout = executeTrecEval(filename+str(mu_range[i]),True)\n",
    "    list_mean_mu.append(getMean(getData(str(stdout),[\"P_1000\",\"recall_1000\",\"ndcg\"])))\n",
    "tuned_mu= mu_range[whoWins(list_mean_mu)]\n",
    "print(\"Best mu for dirichlet_prior\",tuned_mu)\n",
    "\n",
    "delta_range= [0.1,0.4,0.7,0.9]\n",
    "list_mean_delta=[]\n",
    "filename= \"absolute_discount_\"\n",
    "for i in range(len(delta_range)):\n",
    "    stdout = executeTrecEval(filename+str(delta_range[i]),True)\n",
    "    list_mean_delta.append(getMean(getData(str(stdout),[\"P_1000\",\"recall_1000\",\"ndcg\"])))\n",
    "tuned_delta= delta_range[whoWins(list_mean_delta)]\n",
    "\n",
    "mu_PLM_range= [10,40,60,80]\n",
    "list_mean_mu_PLM=[]\n",
    "filename= \"PLM_\"\n",
    "for i in range(len(mu_PLM_range)):\n",
    "    stdout = executeTrecEval(filename+str(mu_PLM_range[i]),True)\n",
    "    list_mean_mu_PLM.append(getMean(getData(str(stdout),[\"P_1000\",\"recall_1000\",\"ndcg\"])))\n",
    "tuned_mu_PLM= mu_PLM_range[whoWins(list_mean_mu_PLM)]\n",
    "print(\"Best mu for PLM\",tuned_mu_PLM)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Implement the above methods and report evaluation measures (on the test set) using the hyper parameter values you optimized on the validation set (also report the values of the hyper parameters). Use TREC Eval to obtain the results and report on `NDCG@10`, Mean Average Precision (`MAP@1000`), `Precision@5` and `Recall@1000`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 /2 600 000\n",
      "1000000 /2 600 000\n",
      "2000000 /2 600 000\n",
      "0 /8200000\n",
      "1000000 /8200000\n",
      "2000000 /8200000\n",
      "3000000 /8200000\n",
      "4000000 /8200000\n",
      "5000000 /8200000\n",
      "6000000 /8200000\n",
      "7000000 /8200000\n",
      "8000000 /8200000\n"
     ]
    }
   ],
   "source": [
    "#DO NOT RUN: EXTREMELY LONG AND MEMORY CONSUMMING : compute the values for the run files (score) for TF_IDF and BM25\n",
    "\n",
    "length = 2\n",
    "#Array of Dictionnary of term to dictionnary of doc_ID to score\n",
    "word_document_score = []\n",
    "for i in range(length):\n",
    "    word_document_score.append({})\n",
    "#Array of Dictionnary of query to tuple (score, document)\n",
    "scores = []\n",
    "for i in range(length):\n",
    "    scores.append({})\n",
    "    \n",
    "prog =0\n",
    "for term_id in query_set:\n",
    "    if term_id not in word_document_score[0]:\n",
    "        for j in range(0,len(scores)):\n",
    "            word_document_score[j][term_id] = {}\n",
    "    for document_id in term_to_doc[term_id]:\n",
    "        if prog%1000000==0:\n",
    "            print(prog,\"/2 600 000\")\n",
    "        prog+=1\n",
    "        #compute scores\n",
    "        word_document_score[0][term_id][document_id]=TF_IDF(term_id, document_id)\n",
    "        word_document_score[1][term_id][document_id]=BM25(term_id, document_id)\n",
    "\n",
    "prog=0\n",
    "            \n",
    "for query_id, term_id_list in query_dict.items():\n",
    "    for j in range(0,len(scores)):\n",
    "        scores[j][query_id] = []\n",
    "        temp_doc_to_score={}\n",
    "        for term_id in term_id_list:\n",
    "            for document_id in term_to_doc[term_id]:\n",
    "                if prog%1000000==0:\n",
    "                    print(prog,\"/\" + str(4100000 * length ))\n",
    "                prog+=1\n",
    "                if document_id not in temp_doc_to_score:\n",
    "                    temp_doc_to_score[document_id]=0\n",
    "                temp_doc_to_score[document_id]+=word_document_score[j][term_id][document_id]\n",
    "        for doc_id in temp_doc_to_score:\n",
    "            scores[j][query_id].append((temp_doc_to_score[doc_id], index.document(doc_id)[0]))\n",
    "        #reset variable to save memory\n",
    "        temp_doc_to_score={}\n",
    "#reset variable to save memory\n",
    "word_document_score= []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#DO NOT RUN (write the run files given the previous scores) for TF_IDF and BM25\n",
    "for j in range(0,len(scores)):\n",
    "    name=\"\"\n",
    "    if(j==0):\n",
    "        name=\"tf_idf\"\n",
    "    elif(j==1):\n",
    "        name=\"BM25\"\n",
    "\n",
    "    write_run(model_name='tf-idf',data=scores[j],out_f=open('./ap_88_89/'+name+'.run', 'w'),max_objects_per_query=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "measure\t\tquery\ttf_idf\tBM25\tjelineM_0.9\tdirichlet_prior_2000\tabsolute_discount_0.4\n",
      "ndcg_cut_10\t100\t0.0000\t0.0000\t0.0000\t\t0.0000\t\t\t0.0000\t\n",
      "ndcg_cut_10\t101\t0.3917\t0.0734\t0.3917\t\t0.3024\t\t\t0.2173\t\n",
      "ndcg_cut_10\t102\t0.0000\t0.0000\t0.0000\t\t0.0000\t\t\t0.0000\t\n",
      "ndcg_cut_10\t104\t0.0000\t0.0000\t0.0000\t\t0.0000\t\t\t0.1389\t\n",
      "ndcg_cut_10\t105\t0.0000\t0.0000\t0.0000\t\t0.0000\t\t\t0.0000\t\n",
      "ndcg_cut_10\t106\t0.0000\t0.0000\t0.0000\t\t0.0000\t\t\t0.0000\t\n",
      "ndcg_cut_10\t107\t0.0734\t0.0000\t0.0734\t\t0.0000\t\t\t0.3629\t\n",
      "ndcg_cut_10\t108\t0.0000\t0.0000\t0.0000\t\t0.0000\t\t\t0.0851\t\n",
      "ndcg_cut_10\t109\t0.0000\t0.0000\t0.0000\t\t0.0000\t\t\t0.0000\t\n",
      "ndcg_cut_10\t110\t0.1478\t0.1100\t0.1428\t\t0.1396\t\t\t0.0663\t\n",
      "ndcg_cut_10\t112\t0.2489\t0.0000\t0.2489\t\t0.1799\t\t\t0.2122\t\n",
      "ndcg_cut_10\t113\t0.0000\t0.0000\t0.0000\t\t0.0000\t\t\t0.0636\t\n",
      "ndcg_cut_10\t115\t0.1389\t0.0000\t0.1389\t\t0.0851\t\t\t0.0000\t\n",
      "ndcg_cut_10\t116\t0.0734\t0.0000\t0.0734\t\t0.0000\t\t\t0.0000\t\n",
      "ndcg_cut_10\t117\t0.0000\t0.0636\t0.0000\t\t0.0000\t\t\t0.0851\t\n",
      "ndcg_cut_10\t118\t0.0000\t0.0000\t0.0000\t\t0.0000\t\t\t0.0663\t\n",
      "ndcg_cut_10\t119\t0.0000\t0.0000\t0.0000\t\t0.0000\t\t\t0.0734\t\n",
      "ndcg_cut_10\t121\t0.0000\t0.0948\t0.0000\t\t0.0000\t\t\t0.0000\t\n",
      "ndcg_cut_10\t122\t0.0000\t0.0000\t0.0000\t\t0.0000\t\t\t0.0000\t\n",
      "ndcg_cut_10\t124\t0.3312\t0.0000\t0.3312\t\t0.0948\t\t\t0.1737\t\n",
      "ndcg_cut_10\t125\t0.6274\t0.3933\t0.5541\t\t0.4537\t\t\t0.2686\t\n",
      "ndcg_cut_10\t126\t0.0784\t0.0000\t0.0784\t\t0.0734\t\t\t0.0000\t\n",
      "ndcg_cut_10\t127\t0.0000\t0.0000\t0.0000\t\t0.0000\t\t\t0.1389\t\n",
      "ndcg_cut_10\t128\t0.0000\t0.0000\t0.0000\t\t0.0000\t\t\t0.3405\t\n",
      "ndcg_cut_10\t129\t0.0000\t0.0000\t0.0000\t\t0.0000\t\t\t0.0000\t\n",
      "ndcg_cut_10\t130\t0.0000\t0.0000\t0.0000\t\t0.0000\t\t\t0.3621\t\n",
      "ndcg_cut_10\t131\t0.0000\t0.0000\t0.0000\t\t0.0000\t\t\t0.0000\t\n",
      "ndcg_cut_10\t132\t0.3693\t0.2489\t0.3667\t\t0.2337\t\t\t0.4920\t\n",
      "ndcg_cut_10\t133\t0.1763\t0.0000\t0.1763\t\t0.1420\t\t\t0.3418\t\n",
      "ndcg_cut_10\t134\t0.2529\t0.0000\t0.2529\t\t0.1089\t\t\t0.4773\t\n",
      "ndcg_cut_10\t136\t0.0000\t0.0000\t0.0000\t\t0.0000\t\t\t0.0000\t\n",
      "ndcg_cut_10\t137\t0.0000\t0.0000\t0.0000\t\t0.0000\t\t\t0.0000\t\n",
      "ndcg_cut_10\t138\t0.0000\t0.0000\t0.0000\t\t0.0000\t\t\t0.0000\t\n",
      "ndcg_cut_10\t139\t0.0000\t0.0000\t0.0000\t\t0.0000\t\t\t0.0784\t\n",
      "ndcg_cut_10\t140\t0.1428\t0.0000\t0.1428\t\t0.1370\t\t\t0.2201\t\n",
      "ndcg_cut_10\t141\t0.0000\t0.0000\t0.0000\t\t0.0000\t\t\t0.0734\t\n",
      "ndcg_cut_10\t142\t0.0000\t0.0000\t0.0000\t\t0.0000\t\t\t0.0694\t\n",
      "ndcg_cut_10\t145\t0.2201\t0.2201\t0.2201\t\t0.2201\t\t\t0.5370\t\n",
      "ndcg_cut_10\t146\t0.0000\t0.0000\t0.0000\t\t0.0000\t\t\t0.1610\t\n",
      "ndcg_cut_10\t147\t0.0000\t0.0000\t0.0000\t\t0.0000\t\t\t0.0000\t\n",
      "ndcg_cut_10\t148\t0.0000\t0.0000\t0.0000\t\t0.0000\t\t\t0.6365\t\n",
      "ndcg_cut_10\t149\t0.0000\t0.0000\t0.0000\t\t0.0000\t\t\t0.1584\t\n",
      "ndcg_cut_10\t150\t0.0000\t0.0000\t0.0000\t\t0.0000\t\t\t0.0000\t\n",
      "ndcg_cut_10\t152\t0.0000\t0.0000\t0.0000\t\t0.0000\t\t\t0.0694\t\n",
      "ndcg_cut_10\t153\t0.3890\t0.0000\t0.3890\t\t0.2966\t\t\t0.3633\t\n",
      "ndcg_cut_10\t154\t0.3977\t0.2083\t0.3341\t\t0.2489\t\t\t0.1331\t\n",
      "ndcg_cut_10\t156\t0.1682\t0.1884\t0.1642\t\t0.2470\t\t\t0.9052\t\n",
      "ndcg_cut_10\t157\t0.3156\t0.0000\t0.3156\t\t0.2221\t\t\t0.6028\t\n",
      "ndcg_cut_10\t159\t0.0000\t0.0000\t0.0000\t\t0.0000\t\t\t0.0000\t\n",
      "ndcg_cut_10\t160\t0.0000\t0.0000\t0.0000\t\t0.0000\t\t\t0.0000\t\n",
      "ndcg_cut_10\t161\t0.7885\t0.6137\t0.7859\t\t0.6489\t\t\t0.0948\t\n",
      "ndcg_cut_10\t162\t0.3301\t0.3689\t0.3301\t\t0.3301\t\t\t0.0636\t\n",
      "ndcg_cut_10\t163\t0.9149\t0.4291\t0.9149\t\t0.7632\t\t\t0.8580\t\n",
      "ndcg_cut_10\t164\t0.5321\t0.1546\t0.5033\t\t0.3188\t\t\t0.2466\t\n",
      "ndcg_cut_10\t166\t0.0000\t0.0000\t0.0000\t\t0.0000\t\t\t0.3278\t\n",
      "ndcg_cut_10\t168\t0.1428\t0.0000\t0.1428\t\t0.1428\t\t\t0.1100\t\n",
      "ndcg_cut_10\t169\t0.0851\t0.0000\t0.0851\t\t0.0851\t\t\t0.0636\t\n",
      "ndcg_cut_10\t171\t0.0000\t0.0000\t0.0000\t\t0.0000\t\t\t0.0000\t\n",
      "ndcg_cut_10\t172\t0.0000\t0.0000\t0.0000\t\t0.0000\t\t\t0.0000\t\n",
      "ndcg_cut_10\t174\t0.2025\t0.1100\t0.2025\t\t0.1389\t\t\t0.6118\t\n",
      "ndcg_cut_10\t175\t0.0694\t0.0000\t0.0663\t\t0.0000\t\t\t0.2906\t\n",
      "ndcg_cut_10\t176\t0.0000\t0.0000\t0.0000\t\t0.0000\t\t\t0.4789\t\n",
      "ndcg_cut_10\t177\t0.0000\t0.0000\t0.0000\t\t0.0000\t\t\t0.0000\t\n",
      "ndcg_cut_10\t178\t0.0734\t0.0000\t0.0734\t\t0.0734\t\t\t0.0000\t\n",
      "ndcg_cut_10\t179\t0.0000\t0.0000\t0.0000\t\t0.0000\t\t\t0.0000\t\n",
      "ndcg_cut_10\t181\t0.0000\t0.0000\t0.0000\t\t0.0000\t\t\t0.2201\t\n",
      "ndcg_cut_10\t183\t0.7543\t0.5424\t0.7543\t\t0.8166\t\t\t1.0000\t\n",
      "ndcg_cut_10\t184\t0.2201\t0.0948\t0.2201\t\t0.2201\t\t\t0.3747\t\n",
      "ndcg_cut_10\t185\t0.0000\t0.0000\t0.0000\t\t0.0000\t\t\t0.0000\t\n",
      "ndcg_cut_10\t186\t0.0000\t0.0000\t0.0000\t\t0.0000\t\t\t0.1389\t\n",
      "ndcg_cut_10\t187\t0.2337\t0.0694\t0.2337\t\t0.1884\t\t\t0.1389\t\n",
      "ndcg_cut_10\t188\t0.0851\t0.0000\t0.0851\t\t0.0784\t\t\t0.6281\t\n",
      "ndcg_cut_10\t189\t0.0000\t0.0000\t0.0000\t\t0.0000\t\t\t0.0734\t\n",
      "ndcg_cut_10\t190\t0.0784\t0.0851\t0.0784\t\t0.0784\t\t\t0.0000\t\n",
      "ndcg_cut_10\t191\t0.0000\t0.0000\t0.0000\t\t0.0000\t\t\t0.0000\t\n",
      "ndcg_cut_10\t193\t0.0000\t0.0000\t0.0000\t\t0.0000\t\t\t0.1389\t\n",
      "ndcg_cut_10\t194\t0.0000\t0.0000\t0.0000\t\t0.0000\t\t\t0.0000\t\n",
      "ndcg_cut_10\t195\t0.0000\t0.0000\t0.0000\t\t0.0000\t\t\t0.0948\t\n",
      "ndcg_cut_10\t196\t0.0000\t0.0000\t0.0000\t\t0.0000\t\t\t0.0000\t\n",
      "ndcg_cut_10\t197\t0.0000\t0.0000\t0.0000\t\t0.0000\t\t\t0.1834\t\n",
      "ndcg_cut_10\t198\t0.2090\t0.0000\t0.2090\t\t0.1370\t\t\t0.2048\t\n",
      "ndcg_cut_10\t199\t0.0000\t0.0000\t0.0000\t\t0.0000\t\t\t0.0000\t\n",
      "ndcg_cut_10\t200\t0.1518\t0.0000\t0.1420\t\t0.1370\t\t\t0.0000\t\n",
      "ndcg_cut_10\t51\t0.7097\t0.2051\t0.7097\t\t0.6154\t\t\t1.0000\t\n",
      "ndcg_cut_10\t52\t0.2685\t0.2048\t0.2685\t\t0.2048\t\t\t0.2547\t\n",
      "ndcg_cut_10\t54\t0.2863\t0.1389\t0.3500\t\t0.2758\t\t\t0.6835\t\n",
      "ndcg_cut_10\t55\t0.2201\t0.2201\t0.2201\t\t0.2201\t\t\t0.1100\t\n",
      "ndcg_cut_10\t56\t0.5389\t0.5638\t0.5389\t\t0.5389\t\t\t1.0000\t\n",
      "ndcg_cut_10\t58\t0.1478\t0.0734\t0.1478\t\t0.1428\t\t\t0.6759\t\n",
      "ndcg_cut_10\t59\t0.0000\t0.0000\t0.0000\t\t0.0000\t\t\t0.4955\t\n",
      "ndcg_cut_10\t60\t0.0000\t0.0000\t0.0000\t\t0.0000\t\t\t0.0000\t\n",
      "ndcg_cut_10\t61\t0.1518\t0.0000\t0.2154\t\t0.0694\t\t\t0.0000\t\n",
      "ndcg_cut_10\t62\t0.0000\t0.0000\t0.0000\t\t0.0000\t\t\t0.0000\t\n",
      "ndcg_cut_10\t63\t0.0000\t0.0000\t0.0000\t\t0.0000\t\t\t0.0000\t\n",
      "ndcg_cut_10\t64\t0.0000\t0.0000\t0.0000\t\t0.0000\t\t\t0.0000\t\n",
      "ndcg_cut_10\t65\t0.0000\t0.0000\t0.0000\t\t0.0000\t\t\t0.0000\t\n",
      "ndcg_cut_10\t66\t0.0000\t0.0000\t0.0000\t\t0.0000\t\t\t0.0000\t\n",
      "ndcg_cut_10\t67\t0.0000\t0.0000\t0.0000\t\t0.0000\t\t\t0.0000\t\n",
      "ndcg_cut_10\t68\t0.0000\t0.0000\t0.0000\t\t0.0000\t\t\t0.2201\t\n",
      "ndcg_cut_10\t70\t0.7255\t0.5469\t0.7255\t\t0.6442\t\t\t0.3827\t\n",
      "ndcg_cut_10\t71\t0.0000\t0.0000\t0.0000\t\t0.0000\t\t\t0.4915\t\n",
      "ndcg_cut_10\t72\t0.0000\t0.0000\t0.0000\t\t0.0000\t\t\t0.0000\t\n",
      "ndcg_cut_10\t73\t0.0000\t0.0000\t0.0000\t\t0.0000\t\t\t0.0000\t\n",
      "ndcg_cut_10\t75\t0.0000\t0.0000\t0.0000\t\t0.0000\t\t\t0.0000\t\n",
      "ndcg_cut_10\t76\t0.0000\t0.0000\t0.0000\t\t0.0000\t\t\t0.0000\t\n",
      "ndcg_cut_10\t77\t0.6036\t0.4131\t0.6036\t\t0.5915\t\t\t0.5290\t\n",
      "ndcg_cut_10\t79\t0.0000\t0.0000\t0.0000\t\t0.0000\t\t\t0.0000\t\n",
      "ndcg_cut_10\t80\t0.0000\t0.0000\t0.0000\t\t0.0000\t\t\t0.0000\t\n",
      "ndcg_cut_10\t81\t0.0000\t0.0000\t0.0000\t\t0.0000\t\t\t0.2240\t\n",
      "ndcg_cut_10\t82\t0.2032\t0.0000\t0.2032\t\t0.0000\t\t\t0.3312\t\n",
      "ndcg_cut_10\t83\t0.0000\t0.0000\t0.0000\t\t0.0000\t\t\t0.3183\t\n",
      "ndcg_cut_10\t84\t0.0663\t0.0000\t0.0636\t\t0.0000\t\t\t0.0000\t\n",
      "ndcg_cut_10\t85\t0.1389\t0.0948\t0.1100\t\t0.1100\t\t\t0.0694\t\n",
      "ndcg_cut_10\t87\t0.0000\t0.0000\t0.0000\t\t0.0000\t\t\t0.0000\t\n",
      "ndcg_cut_10\t88\t0.0000\t0.0000\t0.0000\t\t0.0000\t\t\t0.0000\t\n",
      "ndcg_cut_10\t91\t0.0000\t0.0000\t0.0000\t\t0.0000\t\t\t0.0000\t\n",
      "ndcg_cut_10\t96\t0.0000\t0.0000\t0.0000\t\t0.0000\t\t\t0.0000\t\n",
      "ndcg_cut_10\t97\t0.2863\t0.2837\t0.2863\t\t0.2863\t\t\t0.0734\t\n",
      "ndcg_cut_10\t98\t0.2201\t0.1389\t0.1389\t\t0.1389\t\t\t0.1389\t\n",
      "ndcg_cut_10\t99\t0.1952\t0.0948\t0.1799\t\t0.1799\t\t\t0.0000\t\n",
      "ndcg_cut_10\tall\t0.1181\t0.0587\t0.1165\t\t0.0947\t\t\t0.1776\t\n",
      "map_cut_1000\t100\t0.0119\t0.0032\t0.0114\t\t0.0087\t\t\t0.0238\t\n",
      "map_cut_1000\t101\t0.1373\t0.0354\t0.1361\t\t0.1054\t\t\t0.0886\t\n",
      "map_cut_1000\t102\t0.0128\t0.0042\t0.0127\t\t0.0103\t\t\t0.0276\t\n",
      "map_cut_1000\t104\t0.0205\t0.0067\t0.0203\t\t0.0152\t\t\t0.1131\t\n",
      "map_cut_1000\t105\t0.0000\t0.0000\t0.0000\t\t0.0000\t\t\t0.0008\t\n",
      "map_cut_1000\t106\t0.0356\t0.0069\t0.0351\t\t0.0240\t\t\t0.0076\t\n",
      "map_cut_1000\t107\t0.0500\t0.0063\t0.0486\t\t0.0291\t\t\t0.1564\t\n",
      "map_cut_1000\t108\t0.0417\t0.0142\t0.0417\t\t0.0326\t\t\t0.0139\t\n",
      "map_cut_1000\t109\t0.0007\t0.0015\t0.0007\t\t0.0009\t\t\t0.0000\t\n",
      "map_cut_1000\t110\t0.0686\t0.0155\t0.0677\t\t0.0488\t\t\t0.1607\t\n",
      "map_cut_1000\t112\t0.1667\t0.0560\t0.1667\t\t0.1138\t\t\t0.0697\t\n",
      "map_cut_1000\t113\t0.0033\t0.0003\t0.0029\t\t0.0019\t\t\t0.0225\t\n",
      "map_cut_1000\t115\t0.0268\t0.0088\t0.0261\t\t0.0203\t\t\t0.0564\t\n",
      "map_cut_1000\t116\t0.1053\t0.0422\t0.1032\t\t0.0836\t\t\t0.0964\t\n",
      "map_cut_1000\t117\t0.0858\t0.0270\t0.0856\t\t0.0699\t\t\t0.0344\t\n",
      "map_cut_1000\t118\t0.0036\t0.0020\t0.0036\t\t0.0033\t\t\t0.0379\t\n",
      "map_cut_1000\t119\t0.0074\t0.0027\t0.0074\t\t0.0060\t\t\t0.0312\t\n",
      "map_cut_1000\t121\t0.0036\t0.0072\t0.0036\t\t0.0043\t\t\t0.0123\t\n",
      "map_cut_1000\t122\t0.0310\t0.0016\t0.0292\t\t0.0180\t\t\t0.0125\t\n",
      "map_cut_1000\t124\t0.1543\t0.0496\t0.1541\t\t0.1155\t\t\t0.0176\t\n",
      "map_cut_1000\t125\t0.1020\t0.0381\t0.0979\t\t0.0752\t\t\t0.0390\t\n",
      "map_cut_1000\t126\t0.0107\t0.0017\t0.0106\t\t0.0082\t\t\t0.0371\t\n",
      "map_cut_1000\t127\t0.0003\t0.0000\t0.0003\t\t0.0000\t\t\t0.0131\t\n",
      "map_cut_1000\t128\t0.0019\t0.0012\t0.0018\t\t0.0017\t\t\t0.1225\t\n",
      "map_cut_1000\t129\t0.0002\t0.0001\t0.0002\t\t0.0001\t\t\t0.0143\t\n",
      "map_cut_1000\t130\t0.0268\t0.0095\t0.0266\t\t0.0224\t\t\t0.1505\t\n",
      "map_cut_1000\t131\t0.0355\t0.0238\t0.0354\t\t0.0295\t\t\t0.0164\t\n",
      "map_cut_1000\t132\t0.0811\t0.0315\t0.0802\t\t0.0635\t\t\t0.3508\t\n",
      "map_cut_1000\t133\t0.0823\t0.0272\t0.0818\t\t0.0579\t\t\t0.4151\t\n",
      "map_cut_1000\t134\t0.1300\t0.0035\t0.1299\t\t0.0342\t\t\t0.3469\t\n",
      "map_cut_1000\t136\t0.0186\t0.0045\t0.0182\t\t0.0147\t\t\t0.0850\t\n",
      "map_cut_1000\t137\t0.0193\t0.0070\t0.0191\t\t0.0155\t\t\t0.0378\t\n",
      "map_cut_1000\t138\t0.0148\t0.0031\t0.0147\t\t0.0110\t\t\t0.0255\t\n",
      "map_cut_1000\t139\t0.0055\t0.0048\t0.0055\t\t0.0050\t\t\t0.0378\t\n",
      "map_cut_1000\t140\t0.0189\t0.0060\t0.0186\t\t0.0153\t\t\t0.0633\t\n",
      "map_cut_1000\t141\t0.0144\t0.0029\t0.0141\t\t0.0111\t\t\t0.0301\t\n",
      "map_cut_1000\t142\t0.0007\t0.0001\t0.0007\t\t0.0004\t\t\t0.0032\t\n",
      "map_cut_1000\t145\t0.0428\t0.0192\t0.0427\t\t0.0346\t\t\t0.0456\t\n",
      "map_cut_1000\t146\t0.0049\t0.0005\t0.0049\t\t0.0030\t\t\t0.0608\t\n",
      "map_cut_1000\t147\t0.0095\t0.0010\t0.0095\t\t0.0067\t\t\t0.0003\t\n",
      "map_cut_1000\t148\t0.0017\t0.0012\t0.0017\t\t0.0016\t\t\t0.0482\t\n",
      "map_cut_1000\t149\t0.0126\t0.0105\t0.0126\t\t0.0118\t\t\t0.0647\t\n",
      "map_cut_1000\t150\t0.0304\t0.0046\t0.0303\t\t0.0212\t\t\t0.0106\t\n",
      "map_cut_1000\t152\t0.0131\t0.0018\t0.0131\t\t0.0086\t\t\t0.0148\t\n",
      "map_cut_1000\t153\t0.1210\t0.0188\t0.1204\t\t0.0940\t\t\t0.2227\t\n",
      "map_cut_1000\t154\t0.0634\t0.0243\t0.0629\t\t0.0459\t\t\t0.1483\t\n",
      "map_cut_1000\t156\t0.1331\t0.0303\t0.1321\t\t0.1008\t\t\t0.2228\t\n",
      "map_cut_1000\t157\t0.1323\t0.0290\t0.1316\t\t0.0965\t\t\t0.4523\t\n",
      "map_cut_1000\t159\t0.0157\t0.0008\t0.0156\t\t0.0060\t\t\t0.0311\t\n",
      "map_cut_1000\t160\t0.0017\t0.0002\t0.0017\t\t0.0012\t\t\t0.0048\t\n",
      "map_cut_1000\t161\t0.2562\t0.1072\t0.2543\t\t0.2078\t\t\t0.2612\t\n",
      "map_cut_1000\t162\t0.1108\t0.1299\t0.1104\t\t0.1140\t\t\t0.0417\t\n",
      "map_cut_1000\t163\t0.5560\t0.2000\t0.5500\t\t0.4563\t\t\t0.8501\t\n",
      "map_cut_1000\t164\t0.2414\t0.0552\t0.2305\t\t0.1582\t\t\t0.0643\t\n",
      "map_cut_1000\t166\t0.0077\t0.0027\t0.0077\t\t0.0061\t\t\t0.1404\t\n",
      "map_cut_1000\t168\t0.0361\t0.0251\t0.0360\t\t0.0348\t\t\t0.0126\t\n",
      "map_cut_1000\t169\t0.0736\t0.0209\t0.0729\t\t0.0563\t\t\t0.1042\t\n",
      "map_cut_1000\t171\t0.0066\t0.0016\t0.0065\t\t0.0051\t\t\t0.0429\t\n",
      "map_cut_1000\t172\t0.0144\t0.0036\t0.0143\t\t0.0113\t\t\t0.0106\t\n",
      "map_cut_1000\t174\t0.1917\t0.1296\t0.1901\t\t0.1756\t\t\t0.3395\t\n",
      "map_cut_1000\t175\t0.0734\t0.0133\t0.0725\t\t0.0496\t\t\t0.2178\t\n",
      "map_cut_1000\t176\t0.0002\t0.0000\t0.0002\t\t0.0001\t\t\t0.0278\t\n",
      "map_cut_1000\t177\t0.0826\t0.0149\t0.0818\t\t0.0589\t\t\t0.0486\t\n",
      "map_cut_1000\t178\t0.0501\t0.0446\t0.0501\t\t0.0500\t\t\t0.0096\t\n",
      "map_cut_1000\t179\t0.0004\t0.0002\t0.0004\t\t0.0003\t\t\t0.0035\t\n",
      "map_cut_1000\t181\t0.0007\t0.0000\t0.0007\t\t0.0004\t\t\t0.1062\t\n",
      "map_cut_1000\t183\t0.4362\t0.2616\t0.4348\t\t0.4107\t\t\t0.4465\t\n",
      "map_cut_1000\t184\t0.0731\t0.0142\t0.0726\t\t0.0566\t\t\t0.0665\t\n",
      "map_cut_1000\t185\t0.0302\t0.0051\t0.0300\t\t0.0222\t\t\t0.0092\t\n",
      "map_cut_1000\t186\t0.0068\t0.0006\t0.0067\t\t0.0051\t\t\t0.0574\t\n",
      "map_cut_1000\t187\t0.0419\t0.0230\t0.0417\t\t0.0373\t\t\t0.0154\t\n",
      "map_cut_1000\t188\t0.1219\t0.0906\t0.1219\t\t0.1108\t\t\t0.1929\t\n",
      "map_cut_1000\t189\t0.0915\t0.0445\t0.0915\t\t0.0819\t\t\t0.0152\t\n",
      "map_cut_1000\t190\t0.0434\t0.0221\t0.0434\t\t0.0368\t\t\t0.0228\t\n",
      "map_cut_1000\t191\t0.0001\t0.0000\t0.0001\t\t0.0001\t\t\t0.0000\t\n",
      "map_cut_1000\t193\t0.0520\t0.0373\t0.0519\t\t0.0497\t\t\t0.0425\t\n",
      "map_cut_1000\t194\t0.0039\t0.0005\t0.0038\t\t0.0027\t\t\t0.0030\t\n",
      "map_cut_1000\t195\t0.0064\t0.0002\t0.0061\t\t0.0026\t\t\t0.0454\t\n",
      "map_cut_1000\t196\t0.0138\t0.0005\t0.0135\t\t0.0075\t\t\t0.0270\t\n",
      "map_cut_1000\t197\t0.0019\t0.0003\t0.0019\t\t0.0011\t\t\t0.0196\t\n",
      "map_cut_1000\t198\t0.1389\t0.0584\t0.1386\t\t0.1171\t\t\t0.0519\t\n",
      "map_cut_1000\t199\t0.0006\t0.0006\t0.0006\t\t0.0006\t\t\t0.0004\t\n",
      "map_cut_1000\t200\t0.1067\t0.0104\t0.0985\t\t0.0490\t\t\t0.1716\t\n",
      "map_cut_1000\t51\t0.3577\t0.1896\t0.3559\t\t0.3176\t\t\t0.6380\t\n",
      "map_cut_1000\t52\t0.1759\t0.0637\t0.1744\t\t0.1492\t\t\t0.4860\t\n",
      "map_cut_1000\t54\t0.2458\t0.1774\t0.2457\t\t0.2315\t\t\t0.2456\t\n",
      "map_cut_1000\t55\t0.0510\t0.0135\t0.0505\t\t0.0362\t\t\t0.2194\t\n",
      "map_cut_1000\t56\t0.1547\t0.0684\t0.1533\t\t0.1260\t\t\t0.6473\t\n",
      "map_cut_1000\t58\t0.1351\t0.0780\t0.1347\t\t0.1213\t\t\t0.3429\t\n",
      "map_cut_1000\t59\t0.0052\t0.0014\t0.0052\t\t0.0032\t\t\t0.0390\t\n",
      "map_cut_1000\t60\t0.0023\t0.0000\t0.0023\t\t0.0013\t\t\t0.0106\t\n",
      "map_cut_1000\t61\t0.0711\t0.0069\t0.0703\t\t0.0374\t\t\t0.0888\t\n",
      "map_cut_1000\t62\t0.0026\t0.0008\t0.0026\t\t0.0015\t\t\t0.0113\t\n",
      "map_cut_1000\t63\t0.0144\t0.0061\t0.0144\t\t0.0101\t\t\t0.0144\t\n",
      "map_cut_1000\t64\t0.0202\t0.0024\t0.0199\t\t0.0125\t\t\t0.0137\t\n",
      "map_cut_1000\t65\t0.0000\t0.0000\t0.0000\t\t0.0000\t\t\t0.0000\t\n",
      "map_cut_1000\t66\t0.0000\t0.0000\t0.0000\t\t0.0000\t\t\t0.0000\t\n",
      "map_cut_1000\t67\t0.0000\t0.0001\t0.0000\t\t0.0000\t\t\t0.0007\t\n",
      "map_cut_1000\t68\t0.0034\t0.0014\t0.0034\t\t0.0031\t\t\t0.0587\t\n",
      "map_cut_1000\t70\t0.5445\t0.4509\t0.5445\t\t0.5194\t\t\t0.2248\t\n",
      "map_cut_1000\t71\t0.0341\t0.0164\t0.0341\t\t0.0287\t\t\t0.0636\t\n",
      "map_cut_1000\t72\t0.0000\t0.0000\t0.0000\t\t0.0000\t\t\t0.0000\t\n",
      "map_cut_1000\t73\t0.0000\t0.0000\t0.0000\t\t0.0000\t\t\t0.0000\t\n",
      "map_cut_1000\t75\t0.0568\t0.0456\t0.0568\t\t0.0528\t\t\t0.0433\t\n",
      "map_cut_1000\t76\t0.0000\t0.0000\t0.0000\t\t0.0000\t\t\t0.0012\t\n",
      "map_cut_1000\t77\t0.3256\t0.3214\t0.3256\t\t0.3237\t\t\t0.3012\t\n",
      "map_cut_1000\t79\t0.0005\t0.0000\t0.0005\t\t0.0002\t\t\t0.0009\t\n",
      "map_cut_1000\t80\t0.0000\t0.0000\t0.0000\t\t0.0000\t\t\t0.0018\t\n",
      "map_cut_1000\t81\t0.0269\t0.0064\t0.0265\t\t0.0202\t\t\t0.0632\t\n",
      "map_cut_1000\t82\t0.1066\t0.0597\t0.1064\t\t0.0908\t\t\t0.2638\t\n",
      "map_cut_1000\t83\t0.0054\t0.0009\t0.0054\t\t0.0035\t\t\t0.0311\t\n",
      "map_cut_1000\t84\t0.0115\t0.0047\t0.0109\t\t0.0089\t\t\t0.0156\t\n",
      "map_cut_1000\t85\t0.0168\t0.0033\t0.0164\t\t0.0111\t\t\t0.1006\t\n",
      "map_cut_1000\t87\t0.0002\t0.0001\t0.0002\t\t0.0001\t\t\t0.0013\t\n",
      "map_cut_1000\t88\t0.0059\t0.0005\t0.0059\t\t0.0027\t\t\t0.0151\t\n",
      "map_cut_1000\t91\t0.0000\t0.0000\t0.0000\t\t0.0000\t\t\t0.0044\t\n",
      "map_cut_1000\t96\t0.0014\t0.0001\t0.0014\t\t0.0011\t\t\t0.0597\t\n",
      "map_cut_1000\t97\t0.1133\t0.1011\t0.1133\t\t0.1101\t\t\t0.0585\t\n",
      "map_cut_1000\t98\t0.0682\t0.0333\t0.0403\t\t0.0378\t\t\t0.0442\t\n",
      "map_cut_1000\t99\t0.1027\t0.0302\t0.1011\t\t0.0731\t\t\t0.0771\t\n",
      "map_cut_1000\tall\t0.0637\t0.0300\t0.0629\t\t0.0518\t\t\t0.0980\t\n",
      "P_5\t\t100\t0.0000\t0.0000\t0.0000\t\t0.0000\t\t\t0.0000\t\n",
      "P_5\t\t101\t0.4000\t0.0000\t0.4000\t\t0.4000\t\t\t0.2000\t\n",
      "P_5\t\t102\t0.0000\t0.0000\t0.0000\t\t0.0000\t\t\t0.0000\t\n",
      "P_5\t\t104\t0.0000\t0.0000\t0.0000\t\t0.0000\t\t\t0.2000\t\n",
      "P_5\t\t105\t0.0000\t0.0000\t0.0000\t\t0.0000\t\t\t0.0000\t\n",
      "P_5\t\t106\t0.0000\t0.0000\t0.0000\t\t0.0000\t\t\t0.0000\t\n",
      "P_5\t\t107\t0.0000\t0.0000\t0.0000\t\t0.0000\t\t\t0.2000\t\n",
      "P_5\t\t108\t0.0000\t0.0000\t0.0000\t\t0.0000\t\t\t0.2000\t\n",
      "P_5\t\t109\t0.0000\t0.0000\t0.0000\t\t0.0000\t\t\t0.0000\t\n",
      "P_5\t\t110\t0.0000\t0.2000\t0.0000\t\t0.0000\t\t\t0.0000\t\n",
      "P_5\t\t112\t0.4000\t0.0000\t0.4000\t\t0.4000\t\t\t0.2000\t\n",
      "P_5\t\t113\t0.0000\t0.0000\t0.0000\t\t0.0000\t\t\t0.0000\t\n",
      "P_5\t\t115\t0.2000\t0.0000\t0.2000\t\t0.2000\t\t\t0.0000\t\n",
      "P_5\t\t116\t0.0000\t0.0000\t0.0000\t\t0.0000\t\t\t0.0000\t\n",
      "P_5\t\t117\t0.0000\t0.0000\t0.0000\t\t0.0000\t\t\t0.2000\t\n",
      "P_5\t\t118\t0.0000\t0.0000\t0.0000\t\t0.0000\t\t\t0.0000\t\n",
      "P_5\t\t119\t0.0000\t0.0000\t0.0000\t\t0.0000\t\t\t0.0000\t\n",
      "P_5\t\t121\t0.0000\t0.2000\t0.0000\t\t0.0000\t\t\t0.0000\t\n",
      "P_5\t\t122\t0.0000\t0.0000\t0.0000\t\t0.0000\t\t\t0.0000\t\n",
      "P_5\t\t124\t0.2000\t0.0000\t0.2000\t\t0.2000\t\t\t0.2000\t\n",
      "P_5\t\t125\t0.8000\t0.4000\t0.8000\t\t0.6000\t\t\t0.4000\t\n",
      "P_5\t\t126\t0.0000\t0.0000\t0.0000\t\t0.0000\t\t\t0.0000\t\n",
      "P_5\t\t127\t0.0000\t0.0000\t0.0000\t\t0.0000\t\t\t0.2000\t\n",
      "P_5\t\t128\t0.0000\t0.0000\t0.0000\t\t0.0000\t\t\t0.4000\t\n",
      "P_5\t\t129\t0.0000\t0.0000\t0.0000\t\t0.0000\t\t\t0.0000\t\n",
      "P_5\t\t130\t0.0000\t0.0000\t0.0000\t\t0.0000\t\t\t0.2000\t\n",
      "P_5\t\t131\t0.0000\t0.0000\t0.0000\t\t0.0000\t\t\t0.0000\t\n",
      "P_5\t\t132\t0.4000\t0.4000\t0.4000\t\t0.4000\t\t\t0.4000\t\n",
      "P_5\t\t133\t0.2000\t0.0000\t0.2000\t\t0.0000\t\t\t0.4000\t\n",
      "P_5\t\t134\t0.2000\t0.0000\t0.2000\t\t0.2000\t\t\t0.6000\t\n",
      "P_5\t\t136\t0.0000\t0.0000\t0.0000\t\t0.0000\t\t\t0.0000\t\n",
      "P_5\t\t137\t0.0000\t0.0000\t0.0000\t\t0.0000\t\t\t0.0000\t\n",
      "P_5\t\t138\t0.0000\t0.0000\t0.0000\t\t0.0000\t\t\t0.0000\t\n",
      "P_5\t\t139\t0.0000\t0.0000\t0.0000\t\t0.0000\t\t\t0.0000\t\n",
      "P_5\t\t140\t0.0000\t0.0000\t0.0000\t\t0.0000\t\t\t0.2000\t\n",
      "P_5\t\t141\t0.0000\t0.0000\t0.0000\t\t0.0000\t\t\t0.0000\t\n",
      "P_5\t\t142\t0.0000\t0.0000\t0.0000\t\t0.0000\t\t\t0.0000\t\n",
      "P_5\t\t145\t0.2000\t0.2000\t0.2000\t\t0.2000\t\t\t0.6000\t\n",
      "P_5\t\t146\t0.0000\t0.0000\t0.0000\t\t0.0000\t\t\t0.2000\t\n",
      "P_5\t\t147\t0.0000\t0.0000\t0.0000\t\t0.0000\t\t\t0.0000\t\n",
      "P_5\t\t148\t0.0000\t0.0000\t0.0000\t\t0.0000\t\t\t0.6000\t\n",
      "P_5\t\t149\t0.0000\t0.0000\t0.0000\t\t0.0000\t\t\t0.2000\t\n",
      "P_5\t\t150\t0.0000\t0.0000\t0.0000\t\t0.0000\t\t\t0.0000\t\n",
      "P_5\t\t152\t0.0000\t0.0000\t0.0000\t\t0.0000\t\t\t0.0000\t\n",
      "P_5\t\t153\t0.4000\t0.0000\t0.4000\t\t0.2000\t\t\t0.6000\t\n",
      "P_5\t\t154\t0.6000\t0.2000\t0.6000\t\t0.4000\t\t\t0.0000\t\n",
      "P_5\t\t156\t0.2000\t0.2000\t0.2000\t\t0.2000\t\t\t0.8000\t\n",
      "P_5\t\t157\t0.4000\t0.0000\t0.4000\t\t0.2000\t\t\t0.4000\t\n",
      "P_5\t\t159\t0.0000\t0.0000\t0.0000\t\t0.0000\t\t\t0.0000\t\n",
      "P_5\t\t160\t0.0000\t0.0000\t0.0000\t\t0.0000\t\t\t0.0000\t\n",
      "P_5\t\t161\t1.0000\t0.6000\t1.0000\t\t1.0000\t\t\t0.2000\t\n",
      "P_5\t\t162\t0.4000\t0.4000\t0.4000\t\t0.4000\t\t\t0.0000\t\n",
      "P_5\t\t163\t0.8000\t0.2000\t0.8000\t\t0.8000\t\t\t1.0000\t\n",
      "P_5\t\t164\t0.6000\t0.2000\t0.6000\t\t0.6000\t\t\t0.2000\t\n",
      "P_5\t\t166\t0.0000\t0.0000\t0.0000\t\t0.0000\t\t\t0.4000\t\n",
      "P_5\t\t168\t0.0000\t0.0000\t0.0000\t\t0.0000\t\t\t0.2000\t\n",
      "P_5\t\t169\t0.2000\t0.0000\t0.2000\t\t0.2000\t\t\t0.0000\t\n",
      "P_5\t\t171\t0.0000\t0.0000\t0.0000\t\t0.0000\t\t\t0.0000\t\n",
      "P_5\t\t172\t0.0000\t0.0000\t0.0000\t\t0.0000\t\t\t0.0000\t\n",
      "P_5\t\t174\t0.2000\t0.2000\t0.2000\t\t0.2000\t\t\t0.6000\t\n",
      "P_5\t\t175\t0.0000\t0.0000\t0.0000\t\t0.0000\t\t\t0.2000\t\n",
      "P_5\t\t176\t0.0000\t0.0000\t0.0000\t\t0.0000\t\t\t0.6000\t\n",
      "P_5\t\t177\t0.0000\t0.0000\t0.0000\t\t0.0000\t\t\t0.0000\t\n",
      "P_5\t\t178\t0.0000\t0.0000\t0.0000\t\t0.0000\t\t\t0.0000\t\n",
      "P_5\t\t179\t0.0000\t0.0000\t0.0000\t\t0.0000\t\t\t0.0000\t\n",
      "P_5\t\t181\t0.0000\t0.0000\t0.0000\t\t0.0000\t\t\t0.2000\t\n",
      "P_5\t\t183\t0.8000\t0.6000\t0.8000\t\t0.8000\t\t\t1.0000\t\n",
      "P_5\t\t184\t0.2000\t0.2000\t0.2000\t\t0.2000\t\t\t0.4000\t\n",
      "P_5\t\t185\t0.0000\t0.0000\t0.0000\t\t0.0000\t\t\t0.0000\t\n",
      "P_5\t\t186\t0.0000\t0.0000\t0.0000\t\t0.0000\t\t\t0.2000\t\n",
      "P_5\t\t187\t0.4000\t0.0000\t0.4000\t\t0.2000\t\t\t0.2000\t\n",
      "P_5\t\t188\t0.2000\t0.0000\t0.2000\t\t0.0000\t\t\t0.8000\t\n",
      "P_5\t\t189\t0.0000\t0.0000\t0.0000\t\t0.0000\t\t\t0.0000\t\n",
      "P_5\t\t190\t0.0000\t0.2000\t0.0000\t\t0.0000\t\t\t0.0000\t\n",
      "P_5\t\t191\t0.0000\t0.0000\t0.0000\t\t0.0000\t\t\t0.0000\t\n",
      "P_5\t\t193\t0.0000\t0.0000\t0.0000\t\t0.0000\t\t\t0.2000\t\n",
      "P_5\t\t194\t0.0000\t0.0000\t0.0000\t\t0.0000\t\t\t0.0000\t\n",
      "P_5\t\t195\t0.0000\t0.0000\t0.0000\t\t0.0000\t\t\t0.2000\t\n",
      "P_5\t\t196\t0.0000\t0.0000\t0.0000\t\t0.0000\t\t\t0.0000\t\n",
      "P_5\t\t197\t0.0000\t0.0000\t0.0000\t\t0.0000\t\t\t0.2000\t\n",
      "P_5\t\t198\t0.0000\t0.0000\t0.0000\t\t0.0000\t\t\t0.4000\t\n",
      "P_5\t\t199\t0.0000\t0.0000\t0.0000\t\t0.0000\t\t\t0.0000\t\n",
      "P_5\t\t200\t0.0000\t0.0000\t0.0000\t\t0.0000\t\t\t0.0000\t\n",
      "P_5\t\t51\t0.6000\t0.2000\t0.6000\t\t0.6000\t\t\t1.0000\t\n",
      "P_5\t\t52\t0.4000\t0.4000\t0.4000\t\t0.4000\t\t\t0.2000\t\n",
      "P_5\t\t54\t0.2000\t0.2000\t0.2000\t\t0.2000\t\t\t0.8000\t\n",
      "P_5\t\t55\t0.2000\t0.2000\t0.2000\t\t0.2000\t\t\t0.2000\t\n",
      "P_5\t\t56\t0.8000\t0.8000\t0.8000\t\t0.8000\t\t\t1.0000\t\n",
      "P_5\t\t58\t0.0000\t0.0000\t0.0000\t\t0.0000\t\t\t0.8000\t\n",
      "P_5\t\t59\t0.0000\t0.0000\t0.0000\t\t0.0000\t\t\t0.6000\t\n",
      "P_5\t\t60\t0.0000\t0.0000\t0.0000\t\t0.0000\t\t\t0.0000\t\n",
      "P_5\t\t61\t0.0000\t0.0000\t0.0000\t\t0.0000\t\t\t0.0000\t\n",
      "P_5\t\t62\t0.0000\t0.0000\t0.0000\t\t0.0000\t\t\t0.0000\t\n",
      "P_5\t\t63\t0.0000\t0.0000\t0.0000\t\t0.0000\t\t\t0.0000\t\n",
      "P_5\t\t64\t0.0000\t0.0000\t0.0000\t\t0.0000\t\t\t0.0000\t\n",
      "P_5\t\t65\t0.0000\t0.0000\t0.0000\t\t0.0000\t\t\t0.0000\t\n",
      "P_5\t\t66\t0.0000\t0.0000\t0.0000\t\t0.0000\t\t\t0.0000\t\n",
      "P_5\t\t67\t0.0000\t0.0000\t0.0000\t\t0.0000\t\t\t0.0000\t\n",
      "P_5\t\t68\t0.0000\t0.0000\t0.0000\t\t0.0000\t\t\t0.2000\t\n",
      "P_5\t\t70\t0.8000\t0.6000\t0.8000\t\t0.8000\t\t\t0.2000\t\n",
      "P_5\t\t71\t0.0000\t0.0000\t0.0000\t\t0.0000\t\t\t0.6000\t\n",
      "P_5\t\t72\t0.0000\t0.0000\t0.0000\t\t0.0000\t\t\t0.0000\t\n",
      "P_5\t\t73\t0.0000\t0.0000\t0.0000\t\t0.0000\t\t\t0.0000\t\n",
      "P_5\t\t75\t0.0000\t0.0000\t0.0000\t\t0.0000\t\t\t0.0000\t\n",
      "P_5\t\t76\t0.0000\t0.0000\t0.0000\t\t0.0000\t\t\t0.0000\t\n",
      "P_5\t\t77\t0.6000\t0.6000\t0.6000\t\t0.6000\t\t\t0.4000\t\n",
      "P_5\t\t79\t0.0000\t0.0000\t0.0000\t\t0.0000\t\t\t0.0000\t\n",
      "P_5\t\t80\t0.0000\t0.0000\t0.0000\t\t0.0000\t\t\t0.0000\t\n",
      "P_5\t\t81\t0.0000\t0.0000\t0.0000\t\t0.0000\t\t\t0.4000\t\n",
      "P_5\t\t82\t0.0000\t0.0000\t0.0000\t\t0.0000\t\t\t0.2000\t\n",
      "P_5\t\t83\t0.0000\t0.0000\t0.0000\t\t0.0000\t\t\t0.2000\t\n",
      "P_5\t\t84\t0.0000\t0.0000\t0.0000\t\t0.0000\t\t\t0.0000\t\n",
      "P_5\t\t85\t0.2000\t0.2000\t0.2000\t\t0.2000\t\t\t0.0000\t\n",
      "P_5\t\t87\t0.0000\t0.0000\t0.0000\t\t0.0000\t\t\t0.0000\t\n",
      "P_5\t\t88\t0.0000\t0.0000\t0.0000\t\t0.0000\t\t\t0.0000\t\n",
      "P_5\t\t91\t0.0000\t0.0000\t0.0000\t\t0.0000\t\t\t0.0000\t\n",
      "P_5\t\t96\t0.0000\t0.0000\t0.0000\t\t0.0000\t\t\t0.0000\t\n",
      "P_5\t\t97\t0.2000\t0.2000\t0.2000\t\t0.2000\t\t\t0.0000\t\n",
      "P_5\t\t98\t0.2000\t0.2000\t0.2000\t\t0.2000\t\t\t0.2000\t\n",
      "P_5\t\t99\t0.4000\t0.2000\t0.4000\t\t0.4000\t\t\t0.0000\t\n",
      "P_5\t\tall\t0.1167\t0.0683\t0.1167\t\t0.1050\t\t\t0.1817\t\n",
      "recall_1000\t100\t0.3721\t0.1163\t0.3488\t\t0.3256\t\t\t0.3953\t\n",
      "recall_1000\t101\t0.8519\t0.5556\t0.8519\t\t0.8148\t\t\t1.0000\t\n",
      "recall_1000\t102\t0.5789\t0.4211\t0.5789\t\t0.4737\t\t\t0.6842\t\n",
      "recall_1000\t104\t0.6275\t0.2549\t0.6275\t\t0.5294\t\t\t0.8039\t\n",
      "recall_1000\t105\t0.0000\t0.0000\t0.0000\t\t0.0000\t\t\t0.1212\t\n",
      "recall_1000\t106\t0.5106\t0.3404\t0.5106\t\t0.4681\t\t\t0.3191\t\n",
      "recall_1000\t107\t0.6667\t0.3667\t0.6667\t\t0.6000\t\t\t0.6667\t\n",
      "recall_1000\t108\t0.6170\t0.4043\t0.6170\t\t0.5532\t\t\t0.2553\t\n",
      "recall_1000\t109\t0.2500\t0.2500\t0.2500\t\t0.2500\t\t\t0.0000\t\n",
      "recall_1000\t110\t0.3798\t0.1705\t0.3773\t\t0.3230\t\t\t0.6434\t\n",
      "recall_1000\t112\t0.9167\t0.8333\t0.9167\t\t0.9167\t\t\t0.5833\t\n",
      "recall_1000\t113\t0.2812\t0.0938\t0.2500\t\t0.2188\t\t\t0.4688\t\n",
      "recall_1000\t115\t0.3418\t0.2278\t0.3291\t\t0.3291\t\t\t0.4810\t\n",
      "recall_1000\t116\t0.8824\t0.8235\t0.8824\t\t0.8824\t\t\t1.0000\t\n",
      "recall_1000\t117\t0.8621\t0.5517\t0.8621\t\t0.8621\t\t\t0.5862\t\n",
      "recall_1000\t118\t0.1263\t0.0909\t0.1263\t\t0.1212\t\t\t0.2778\t\n",
      "recall_1000\t119\t0.1464\t0.0921\t0.1464\t\t0.1339\t\t\t0.3222\t\n",
      "recall_1000\t121\t0.1250\t0.1250\t0.1250\t\t0.1250\t\t\t0.3750\t\n",
      "recall_1000\t122\t0.7500\t0.2500\t0.7500\t\t0.6000\t\t\t0.4000\t\n",
      "recall_1000\t124\t0.8103\t0.6897\t0.8103\t\t0.7931\t\t\t0.2414\t\n",
      "recall_1000\t125\t0.3789\t0.2632\t0.3789\t\t0.3368\t\t\t0.3579\t\n",
      "recall_1000\t126\t0.2126\t0.1034\t0.2126\t\t0.1954\t\t\t0.4598\t\n",
      "recall_1000\t127\t0.0485\t0.0000\t0.0485\t\t0.0182\t\t\t0.2485\t\n",
      "recall_1000\t128\t0.1786\t0.1607\t0.1786\t\t0.1607\t\t\t0.6607\t\n",
      "recall_1000\t129\t0.0419\t0.0180\t0.0419\t\t0.0299\t\t\t0.2216\t\n",
      "recall_1000\t130\t0.2982\t0.1842\t0.2982\t\t0.2719\t\t\t0.4956\t\n",
      "recall_1000\t131\t0.6667\t0.5000\t0.6667\t\t0.6667\t\t\t0.8333\t\n",
      "recall_1000\t132\t0.5985\t0.3358\t0.5985\t\t0.5474\t\t\t0.7445\t\n",
      "recall_1000\t133\t1.0000\t0.9231\t1.0000\t\t1.0000\t\t\t1.0000\t\n",
      "recall_1000\t134\t0.7500\t0.2500\t0.7500\t\t0.6250\t\t\t0.8750\t\n",
      "recall_1000\t136\t0.9000\t0.5000\t0.9000\t\t0.9000\t\t\t1.0000\t\n",
      "recall_1000\t137\t0.4074\t0.2778\t0.4074\t\t0.3704\t\t\t0.4630\t\n",
      "recall_1000\t138\t0.5833\t0.3056\t0.5833\t\t0.5278\t\t\t0.6667\t\n",
      "recall_1000\t139\t0.3617\t0.3191\t0.3617\t\t0.3191\t\t\t0.5106\t\n",
      "recall_1000\t140\t0.3200\t0.1600\t0.2800\t\t0.2000\t\t\t0.5600\t\n",
      "recall_1000\t141\t0.4545\t0.1818\t0.4545\t\t0.4091\t\t\t0.7273\t\n",
      "recall_1000\t142\t0.0446\t0.0149\t0.0417\t\t0.0387\t\t\t0.0833\t\n",
      "recall_1000\t145\t0.5243\t0.3010\t0.5243\t\t0.4660\t\t\t0.2330\t\n",
      "recall_1000\t146\t0.1437\t0.0469\t0.1437\t\t0.1125\t\t\t0.2875\t\n",
      "recall_1000\t147\t0.2627\t0.0847\t0.2627\t\t0.2288\t\t\t0.0424\t\n",
      "recall_1000\t148\t0.0909\t0.0773\t0.0909\t\t0.0909\t\t\t0.2182\t\n",
      "recall_1000\t149\t0.3333\t0.3000\t0.3333\t\t0.3333\t\t\t0.2667\t\n",
      "recall_1000\t150\t0.3008\t0.1271\t0.3008\t\t0.2712\t\t\t0.1780\t\n",
      "recall_1000\t152\t0.1994\t0.0772\t0.1994\t\t0.1672\t\t\t0.1801\t\n",
      "recall_1000\t153\t0.7297\t0.4324\t0.7297\t\t0.6486\t\t\t0.7838\t\n",
      "recall_1000\t154\t0.3622\t0.2378\t0.3622\t\t0.3089\t\t\t0.3422\t\n",
      "recall_1000\t156\t0.4727\t0.2667\t0.4727\t\t0.4394\t\t\t0.5061\t\n",
      "recall_1000\t157\t0.9500\t0.6000\t0.9500\t\t0.7500\t\t\t1.0000\t\n",
      "recall_1000\t159\t0.6000\t0.4000\t0.6000\t\t0.6000\t\t\t0.6000\t\n",
      "recall_1000\t160\t0.1538\t0.0769\t0.1538\t\t0.1538\t\t\t0.2308\t\n",
      "recall_1000\t161\t0.6917\t0.5489\t0.6917\t\t0.6617\t\t\t0.9850\t\n",
      "recall_1000\t162\t0.5000\t0.5000\t0.5000\t\t0.5000\t\t\t0.4432\t\n",
      "recall_1000\t163\t0.9103\t0.7821\t0.9103\t\t0.8846\t\t\t0.9872\t\n",
      "recall_1000\t164\t0.7353\t0.7059\t0.7353\t\t0.7353\t\t\t0.4412\t\n",
      "recall_1000\t166\t0.5263\t0.2632\t0.5263\t\t0.4737\t\t\t0.7368\t\n",
      "recall_1000\t168\t0.4603\t0.4921\t0.4603\t\t0.4762\t\t\t0.3016\t\n",
      "recall_1000\t169\t0.6562\t0.5000\t0.6562\t\t0.5938\t\t\t0.8125\t\n",
      "recall_1000\t171\t0.7143\t0.4286\t0.7143\t\t0.7143\t\t\t0.8571\t\n",
      "recall_1000\t172\t0.6250\t0.6250\t0.6250\t\t0.6250\t\t\t0.6250\t\n",
      "recall_1000\t174\t0.9036\t0.8795\t0.9036\t\t0.9036\t\t\t0.9880\t\n",
      "recall_1000\t175\t0.6471\t0.4412\t0.6471\t\t0.5882\t\t\t0.8529\t\n",
      "recall_1000\t176\t0.0357\t0.0000\t0.0357\t\t0.0286\t\t\t0.1714\t\n",
      "recall_1000\t177\t0.5679\t0.3086\t0.5679\t\t0.4938\t\t\t0.7037\t\n",
      "recall_1000\t178\t0.7000\t0.7000\t0.7000\t\t0.7000\t\t\t0.5333\t\n",
      "recall_1000\t179\t0.0976\t0.0488\t0.0976\t\t0.0732\t\t\t0.1707\t\n",
      "recall_1000\t181\t0.2000\t0.0000\t0.2000\t\t0.2000\t\t\t0.5000\t\n",
      "recall_1000\t183\t0.8361\t0.8033\t0.8361\t\t0.8361\t\t\t0.6885\t\n",
      "recall_1000\t184\t0.5227\t0.3636\t0.5227\t\t0.4773\t\t\t0.5227\t\n",
      "recall_1000\t185\t0.4095\t0.2190\t0.4095\t\t0.3810\t\t\t0.2762\t\n",
      "recall_1000\t186\t0.3488\t0.1163\t0.3488\t\t0.3488\t\t\t0.5581\t\n",
      "recall_1000\t187\t0.3537\t0.2857\t0.3537\t\t0.3537\t\t\t0.2381\t\n",
      "recall_1000\t188\t0.8657\t0.8657\t0.8657\t\t0.8657\t\t\t0.8657\t\n",
      "recall_1000\t189\t0.2993\t0.1950\t0.2993\t\t0.2812\t\t\t0.1100\t\n",
      "recall_1000\t190\t0.7742\t0.6129\t0.7742\t\t0.7097\t\t\t0.7419\t\n",
      "recall_1000\t191\t0.0157\t0.0079\t0.0157\t\t0.0157\t\t\t0.0157\t\n",
      "recall_1000\t193\t0.4417\t0.4500\t0.4417\t\t0.4417\t\t\t0.2833\t\n",
      "recall_1000\t194\t0.1579\t0.0789\t0.1579\t\t0.1579\t\t\t0.1579\t\n",
      "recall_1000\t195\t0.2500\t0.0461\t0.2434\t\t0.1579\t\t\t0.4013\t\n",
      "recall_1000\t196\t0.5000\t0.1429\t0.5000\t\t0.4643\t\t\t0.4286\t\n",
      "recall_1000\t197\t0.1587\t0.0476\t0.1587\t\t0.1111\t\t\t0.2381\t\n",
      "recall_1000\t198\t0.6974\t0.6053\t0.6974\t\t0.6974\t\t\t0.3026\t\n",
      "recall_1000\t199\t0.2000\t0.2000\t0.2000\t\t0.2000\t\t\t0.1333\t\n",
      "recall_1000\t200\t0.6923\t0.5128\t0.6923\t\t0.6410\t\t\t0.7949\t\n",
      "recall_1000\t51\t0.9688\t0.9375\t0.9688\t\t0.9375\t\t\t0.9062\t\n",
      "recall_1000\t52\t0.6655\t0.4676\t0.6655\t\t0.6547\t\t\t0.8201\t\n",
      "recall_1000\t54\t0.8696\t0.7935\t0.8696\t\t0.8696\t\t\t0.8587\t\n",
      "recall_1000\t55\t0.3786\t0.1770\t0.3786\t\t0.3169\t\t\t0.9053\t\n",
      "recall_1000\t56\t0.5246\t0.3732\t0.5246\t\t0.4859\t\t\t0.8908\t\n",
      "recall_1000\t58\t0.7273\t0.7172\t0.7273\t\t0.7374\t\t\t0.6970\t\n",
      "recall_1000\t59\t0.1045\t0.0473\t0.1045\t\t0.0828\t\t\t0.1795\t\n",
      "recall_1000\t60\t0.4444\t0.0000\t0.4444\t\t0.3333\t\t\t0.4444\t\n",
      "recall_1000\t61\t0.5821\t0.2836\t0.5821\t\t0.4925\t\t\t0.6866\t\n",
      "recall_1000\t62\t0.1189\t0.0573\t0.1189\t\t0.0837\t\t\t0.1101\t\n",
      "recall_1000\t63\t0.6000\t0.6000\t0.6000\t\t0.6000\t\t\t0.4000\t\n",
      "recall_1000\t64\t0.2526\t0.0947\t0.2491\t\t0.2070\t\t\t0.1825\t\n",
      "recall_1000\t65\t0.0000\t0.0000\t0.0000\t\t0.0000\t\t\t0.0000\t\n",
      "recall_1000\t66\t0.0000\t0.0000\t0.0000\t\t0.0000\t\t\t0.0000\t\n",
      "recall_1000\t67\t0.0068\t0.0090\t0.0068\t\t0.0068\t\t\t0.0317\t\n",
      "recall_1000\t68\t0.2128\t0.1489\t0.2128\t\t0.2128\t\t\t0.5106\t\n",
      "recall_1000\t70\t1.0000\t1.0000\t1.0000\t\t1.0000\t\t\t1.0000\t\n",
      "recall_1000\t71\t0.3442\t0.2565\t0.3442\t\t0.3214\t\t\t0.1721\t\n",
      "recall_1000\t72\t0.0000\t0.0000\t0.0000\t\t0.0000\t\t\t0.0000\t\n",
      "recall_1000\t73\t0.0094\t0.0000\t0.0094\t\t0.0000\t\t\t0.0094\t\n",
      "recall_1000\t75\t0.7692\t0.7692\t0.7692\t\t0.7692\t\t\t0.7692\t\n",
      "recall_1000\t76\t0.0145\t0.0145\t0.0145\t\t0.0290\t\t\t0.0725\t\n",
      "recall_1000\t77\t0.5739\t0.5739\t0.5739\t\t0.5739\t\t\t0.5739\t\n",
      "recall_1000\t79\t0.0625\t0.0156\t0.0625\t\t0.0391\t\t\t0.0625\t\n",
      "recall_1000\t80\t0.0119\t0.0060\t0.0119\t\t0.0119\t\t\t0.0893\t\n",
      "recall_1000\t81\t0.4906\t0.3208\t0.4906\t\t0.4906\t\t\t0.6981\t\n",
      "recall_1000\t82\t0.6341\t0.5691\t0.6341\t\t0.6098\t\t\t0.5285\t\n",
      "recall_1000\t83\t0.1417\t0.0648\t0.1417\t\t0.1215\t\t\t0.2429\t\n",
      "recall_1000\t84\t0.5000\t0.3182\t0.5000\t\t0.5000\t\t\t0.4545\t\n",
      "recall_1000\t85\t0.1498\t0.0619\t0.1498\t\t0.1238\t\t\t0.3876\t\n",
      "recall_1000\t87\t0.0577\t0.0385\t0.0577\t\t0.0385\t\t\t0.1346\t\n",
      "recall_1000\t88\t0.3333\t0.0758\t0.3333\t\t0.2121\t\t\t0.5455\t\n",
      "recall_1000\t91\t0.0000\t0.0000\t0.0000\t\t0.0000\t\t\t0.4000\t\n",
      "recall_1000\t96\t0.2500\t0.0833\t0.2500\t\t0.2500\t\t\t0.8333\t\n",
      "recall_1000\t97\t0.7000\t0.8000\t0.7000\t\t0.7000\t\t\t0.7000\t\n",
      "recall_1000\t98\t0.6111\t0.4444\t0.6111\t\t0.5556\t\t\t0.4444\t\n",
      "recall_1000\t99\t0.6185\t0.3526\t0.6185\t\t0.5434\t\t\t0.7110\t\n",
      "recall_1000\tall\t0.4391\t0.3119\t0.4381\t\t0.4101\t\t\t0.4810\t\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#display measures\n",
    "filenames=[\"tf_idf\",\"BM25\",(\"jelineM_\"+str(tuned_lambda)),(\"dirichlet_prior_\"+str(tuned_mu)),\n",
    "           (\"absolute_discount_\"+str(tuned_delta))]\n",
    "nb_files = len(filenames)\n",
    "#ndcg_cut_10 , map_cut_1000, P_5, recall_1000\n",
    "tab_result=[]\n",
    "for file_num in range(len(filenames)):\n",
    "    filename=filenames[file_num]\n",
    "    stdout = executeTrecEval(filename,False)\n",
    "    data = getData(str(stdout),[\"ndcg_cut_10\",\"map_cut_1000\",\"P_5\",\"recall_1000\"])\n",
    "    for i in range(len(data)):\n",
    "        foundQuery=False\n",
    "        for j in range(len(tab_result)):\n",
    "            if tab_result[j][0]==data[i][0]:\n",
    "                if(tab_result[j][1]==data[i][1]):\n",
    "                    foundQuery=True\n",
    "                    tab_result[j][2+file_num]=str(data[i][2])\n",
    "                    break\n",
    "        if not foundQuery:\n",
    "            tab_result.append([])\n",
    "            for k in range(nb_files+2):\n",
    "                tab_result[len(tab_result)-1].append(\"\")\n",
    "            tab_result[len(tab_result)-1][0]=data[i][0]\n",
    "            tab_result[len(tab_result)-1][1]=data[i][1]\n",
    "            tab_result[len(tab_result)-1][2+file_num]=data[i][2]\n",
    "\n",
    "res_s=\"measure\\t\\tquery\"\n",
    "for filename in filenames:\n",
    "    res_s+=\"\\t\"+filename\n",
    "res_s+=\"\\n\"\n",
    "for line in tab_result:\n",
    "    for i in range(len(line)):\n",
    "        element= line[i]\n",
    "        shift = \"\\t\"\n",
    "        if i==0 and line[0]==\"P_5\" : \n",
    "            shift+=\"\\t\"\n",
    "        if i==4 : \n",
    "            shift+=\"\\t\"\n",
    "        if i==5 : \n",
    "            shift+=\"\\t\"+\"\\t\"\n",
    "        res_s+=element+shift\n",
    "    res_s+=\"\\n\"\n",
    "print(res_s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) For the language models, create plots showing `NDCG@10` with varying values of the parameters. You can do this by chaining small scripts using shell scripting (preferred) or execute trec_eval using Python's `subprocess`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Unable to do that since PLM is too slow "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5) Compute significance of the results using a [two-tailed paired Student t-test](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.ttest_rel.html) **[10 points]**. Be wary of false rejection of the null hypothesis caused by the [multiple comparisons problem](https://en.wikipedia.org/wiki/Multiple_comparisons_problem). There are multiple ways to mitigate this problem and it is up to you to choose one.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf_idf BM25 ndcg_cut_10 \t\tp_value: 3.0193923225e-08 \t correlation: True\n",
      "tf_idf BM25 map_cut_1000 \t\tp_value: 4.91958186193e-11 \t correlation: True\n",
      "tf_idf BM25 P_5 \t\tp_value: 0.000142941298149 \t correlation: True\n",
      "tf_idf BM25 recall_1000 \t\tp_value: 1.78188189444e-21 \t correlation: True\n",
      "tf_idf jelineM_0.9 ndcg_cut_10 \t\tp_value: 0.233028447199 \t correlation: False\n",
      "tf_idf jelineM_0.9 map_cut_1000 \t\tp_value: 0.00351157193189 \t correlation: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/scipy/stats/stats.py:3658: RuntimeWarning: invalid value encountered in absolute\n",
      "  prob = distributions.t.sf(np.abs(t), df) * 2  # use np.abs to get upper tail\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf_idf jelineM_0.9 P_5 \t\tp_value: nan \t correlation: False\n",
      "tf_idf jelineM_0.9 recall_1000 \t\tp_value: 0.0333853395991 \t correlation: False\n",
      "tf_idf dirichlet_prior_2000 ndcg_cut_10 \t\tp_value: 2.44684117711e-06 \t correlation: True\n",
      "tf_idf dirichlet_prior_2000 map_cut_1000 \t\tp_value: 6.60966318681e-11 \t correlation: True\n",
      "tf_idf dirichlet_prior_2000 P_5 \t\tp_value: 0.00761212575597 \t correlation: False\n",
      "tf_idf dirichlet_prior_2000 recall_1000 \t\tp_value: 5.2915125359e-14 \t correlation: True\n",
      "tf_idf absolute_discount_0.4 ndcg_cut_10 \t\tp_value: 0.00191109110624 \t correlation: True\n",
      "tf_idf absolute_discount_0.4 map_cut_1000 \t\tp_value: 0.000294841317932 \t correlation: True\n",
      "tf_idf absolute_discount_0.4 P_5 \t\tp_value: 0.00477037742187 \t correlation: False\n",
      "tf_idf absolute_discount_0.4 recall_1000 \t\tp_value: 0.014110371994 \t correlation: False\n",
      "BM25 jelineM_0.9 ndcg_cut_10 \t\tp_value: 5.42805312297e-08 \t correlation: True\n",
      "BM25 jelineM_0.9 map_cut_1000 \t\tp_value: 6.32359716957e-11 \t correlation: True\n",
      "BM25 jelineM_0.9 P_5 \t\tp_value: 0.000142941298149 \t correlation: True\n",
      "BM25 jelineM_0.9 recall_1000 \t\tp_value: 2.4741773028e-21 \t correlation: True\n",
      "BM25 dirichlet_prior_2000 ndcg_cut_10 \t\tp_value: 1.84180643497e-06 \t correlation: True\n",
      "BM25 dirichlet_prior_2000 map_cut_1000 \t\tp_value: 5.93227859973e-10 \t correlation: True\n",
      "BM25 dirichlet_prior_2000 P_5 \t\tp_value: 0.000737001884976 \t correlation: True\n",
      "BM25 dirichlet_prior_2000 recall_1000 \t\tp_value: 4.12249493067e-20 \t correlation: True\n",
      "BM25 absolute_discount_0.4 ndcg_cut_10 \t\tp_value: 1.31658796543e-08 \t correlation: True\n",
      "BM25 absolute_discount_0.4 map_cut_1000 \t\tp_value: 2.35831119852e-08 \t correlation: True\n",
      "BM25 absolute_discount_0.4 P_5 \t\tp_value: 1.57785770088e-06 \t correlation: True\n",
      "BM25 absolute_discount_0.4 recall_1000 \t\tp_value: 3.7093728131e-13 \t correlation: True\n",
      "jelineM_0.9 dirichlet_prior_2000 ndcg_cut_10 \t\tp_value: 4.26600431614e-06 \t correlation: True\n",
      "jelineM_0.9 dirichlet_prior_2000 map_cut_1000 \t\tp_value: 9.70799347485e-11 \t correlation: True\n",
      "jelineM_0.9 dirichlet_prior_2000 P_5 \t\tp_value: 0.00761212575597 \t correlation: False\n",
      "jelineM_0.9 dirichlet_prior_2000 recall_1000 \t\tp_value: 9.38115391827e-14 \t correlation: True\n",
      "jelineM_0.9 absolute_discount_0.4 ndcg_cut_10 \t\tp_value: 0.0012273624724 \t correlation: True\n",
      "jelineM_0.9 absolute_discount_0.4 map_cut_1000 \t\tp_value: 0.00021743534675 \t correlation: True\n",
      "jelineM_0.9 absolute_discount_0.4 P_5 \t\tp_value: 0.00477037742187 \t correlation: False\n",
      "jelineM_0.9 absolute_discount_0.4 recall_1000 \t\tp_value: 0.0123049748836 \t correlation: False\n",
      "dirichlet_prior_2000 absolute_discount_0.4 ndcg_cut_10 \t\tp_value: 7.23558628247e-06 \t correlation: True\n",
      "dirichlet_prior_2000 absolute_discount_0.4 map_cut_1000 \t\tp_value: 5.67042300011e-06 \t correlation: True\n",
      "dirichlet_prior_2000 absolute_discount_0.4 P_5 \t\tp_value: 0.000942234232302 \t correlation: True\n",
      "dirichlet_prior_2000 absolute_discount_0.4 recall_1000 \t\tp_value: 8.72269733947e-05 \t correlation: True\n"
     ]
    }
   ],
   "source": [
    "import scipy as sc\n",
    "import scipy.stats as stats\n",
    "\n",
    "filenames=[\"tf_idf\",\"BM25\",(\"jelineM_\"+str(tuned_lambda)),(\"dirichlet_prior_\"+str(tuned_mu)),\n",
    "           (\"absolute_discount_\"+str(tuned_delta))]\n",
    "measures=[\"ndcg_cut_10\",\"map_cut_1000\",\"P_5\",\"recall_1000\"]\n",
    "\n",
    "def runttest(method1,method2,measure):\n",
    "    stdout1 = executeTrecEval(method1,False)\n",
    "    data1 = getData(str(stdout1),[measure])\n",
    "    stdout2 = executeTrecEval(method2,False)\n",
    "    data2 = getData(str(stdout2),[measure])\n",
    "    data_array_1=[]\n",
    "    data_array_2=[]\n",
    "    for i in range(len(data1)-1):\n",
    "        data_array_1.append(float(data1[i][2]))\n",
    "    for j in range(len(data2)-1):\n",
    "        data_array_2.append(float(data2[j][2]))\n",
    "    return stats.ttest_rel(data_array_1,data_array_2)\n",
    "\n",
    "hypo_num=0;\n",
    "for i in range(0,len(filenames)):\n",
    "    for j in range(i+1,len(filenames)):\n",
    "        for k in range(len(measures)):\n",
    "            hypo_num+=1\n",
    "            p_value= str(runttest(filenames[i],filenames[j],measures[k])[1])\n",
    "            hypo_valid= float(p_value) < 0.05/hypo_num\n",
    "            print(filenames[i],filenames[j],measures[k],\"\\t\\tp_value: \"+str(p_value),\"\\t correlation: \"+ str(hypo_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6) Analyse the results by identifying specific queries where different methods succeed or fail and discuss possible reasons that cause these differences.\n",
    "\n",
    "__TODO__\n",
    "\n",
    "absolute discount seems better on ndcg\n",
    "query 105, 191 ndcg: \"Black Monday\" , Efforts to Improve U.S. Schooling\n",
    "query 76 map cut : U.S. Constitution - Original Intent\n",
    "query 70 p_5: Surrogate Motherhood\n",
    "recall 1000: 112 Funding Biotechnology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Latent Semantic Models (LSMs) [25 points + 10 bonus points] ###\n",
    "\n",
    "In this task you will experiment with applying a distributional semantics methods ([word2vec](http://arxiv.org/abs/1411.2738)  **[5 points]**, [LSI](http://lsa3.colorado.edu/papers/JASIS.lsi.90.pdf) **[5 points]**, [LDA](https://www.cs.princeton.edu/~blei/papers/BleiNgJordan2003.pdf) **[5 points]** and [doc2vec](https://cs.stanford.edu/~quocle/paragraph_vector.pdf) **[5 points]**) for retrieval.\n",
    "\n",
    "You do not need to implement word2vec, LSI, LDA and doc2vec on your own. Instead, you can use [gensim](http://radimrehurek.com/gensim/index.html) (pre-loaded on the VirtualBox). An example on how to integrate Pyndri with Gensim for word2vec can be found [here](https://github.com/cvangysel/pyndri/blob/master/examples/word2vec.py). For the remaining latent vector space models, you will need to implement connector classes (such as `IndriSentences`) by yourself.\n",
    "\n",
    "In order to use a latent semantic model for retrieval, you need to:\n",
    "   * build a representation of the query **q**,\n",
    "   * build a representation of the document **d**,\n",
    "   * calculate the similarity between **q** and **d** (e.g., cosine similarity, KL-divergence).\n",
    "     \n",
    "The exact implementation here depends on the latent semantic model you are using. For example, in the case of word2vec, you only have vectors for individual words and not for documents or phrases. Try one of the following methods for producing these representations:\n",
    "   * Average or sum the word vectors.\n",
    "   * Cluster words in the document using [k-means](http://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html) and use the centroid of the most important cluster. Experiment with different values of K for k-means.\n",
    "   * Using the [bag-of-word-embeddings representation](https://ciir-publications.cs.umass.edu/pub/web/getpdf.php?id=1248). **[10 bonus points]**\n",
    "   \n",
    "Each of these LSMs come with various hyperparameters to tune. Make a choice on the parameters, and explicitly mention the reasons that led you to these decisions. You can use the validation set to optimize hyper parameters you see fit; motivate your decisions. In addition, mention clearly how the query/document representations were constructed for each LSM and explain your choices.\n",
    "\n",
    "In this experiment, you will first obtain an initial top-1000 ranking for each query using TF-IDF in **Task 1**, and then re-rank the documents using the LSMs. Use TREC Eval to obtain the results and report on `NDCG@10`, Mean Average Precision (`MAP@1000`), `Precision@5` and `Recall@1000`.\n",
    "\n",
    "Perform significance testing **[5 points]** (similar as in Task 1) in the class of semantic matching methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Pre Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 /4100000\n",
      "1000000 /4100000\n",
      "2000000 /4100000\n",
      "3000000 /4100000\n",
      "4000000 /4100000\n",
      "5000000 /4100000\n",
      "6000000 /4100000\n",
      "7000000 /4100000\n",
      "8000000 /4100000\n",
      "9000000 /4100000\n",
      "10000000 /4100000\n",
      "11000000 /4100000\n"
     ]
    }
   ],
   "source": [
    "# getting 1000 documents given query for task 2\n",
    "scores = {}\n",
    "prog = 0\n",
    "for query_id, term_id_list in query_dict.items():\n",
    "        scores[query_id] = []\n",
    "        document_set=set()\n",
    "        for term_id in term_id_list:\n",
    "            document_set = document_set | set(term_to_doc[term_id])\n",
    "            for document_id in document_set:\n",
    "                if prog%1000000==0:\n",
    "                    print(prog,\"/\" + str(4100000))\n",
    "                prog+=1\n",
    "                # for task 2, i need the int document_id\n",
    "                scores[query_id].append((TF_IDF(term_id,document_id),index.document(document_id)[0],document_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf_idf_scores = {}\n",
    "for key in scores:\n",
    "    tf_idf_scores[key] = []\n",
    "    for elem in scores[key]:\n",
    "        lst = list(elem)\n",
    "        lst = lst[:-1]\n",
    "        tf_idf_scores[key].append(tuple(lst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# sort the result and get the 1000 first documents\n",
    "from operator import itemgetter\n",
    "sorted_query_doc = {}\n",
    "for query_id, term_id_list in query_dict.items():\n",
    "    sorted_query_doc[query_id] = sorted(scores[query_id],key=itemgetter(0), reverse=True)\n",
    "    del sorted_query_doc[query_id][1000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# prepare documents\n",
    "import copy\n",
    "import gensim\n",
    "import logging\n",
    "import pyndri\n",
    "import pyndri.compat\n",
    "import sys\n",
    "\n",
    "logging.info('Loading vocabulary.')\n",
    "doc = index.document(index.document_base())\n",
    "dictionary = pyndri.extract_dictionary(index)\n",
    "sentences = pyndri.compat.IndriSentences(index, dictionary)\n",
    "\n",
    "logging.info('Constructing word2vec vocabulary.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TAKES A LOT OF TIME. I SAVED THE TRAINED MODEL ON DISK\n",
    "# WORD2VEC, sentences == documents in collections\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "logging.info('Initializing word2vec.')\n",
    "\n",
    "word2vec_init = gensim.models.Word2Vec(\n",
    "    size=32,  # Embedding size\n",
    "    window=5,  # One-sided window size\n",
    "    sg=True,  # Skip-gram.\n",
    "    min_count=5,  # Minimum word frequency.\n",
    "    sample=1e-3,  # Sub-sample threshold.\n",
    "    hs=False,  # Hierarchical softmax.\n",
    "    negative=10,  # Number of negative examples.\n",
    "    iter=1,  # Number of iterations.\n",
    "    workers=8,  # Number of workers.\n",
    ")\n",
    "\n",
    "word2vec_init.build_vocab(sentences, trim_rule=None)\n",
    "\n",
    "models = [word2vec_init]\n",
    "for epoch in range(1, 5 + 1):\n",
    "    logging.info('Epoch %d', epoch)\n",
    "\n",
    "    model = copy.deepcopy(models[-1])\n",
    "    model.train(sentences)\n",
    "\n",
    "    models.append(model)\n",
    "\n",
    "logging.info('Trained models: %s', models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gensim.utils:saving Word2Vec object under skip-gram_32embeddings.model, separately None\n",
      "INFO:gensim.utils:not storing attribute syn0norm\n",
      "INFO:gensim.utils:not storing attribute cum_table\n",
      "INFO:gensim.utils:saved skip-gram_32embeddings.model\n"
     ]
    }
   ],
   "source": [
    "# save trained model to disk\n",
    "model.save(\"skip-gram_32embeddings.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load existing model from disk\n",
    "model = gensim.models.Word2Vec.load(\"skip-gram_32embeddings.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics.cluster import mutual_info_score \n",
    "word2vec_cosine_sim = {}\n",
    "word2vec_kl_div = {}\n",
    "# create representation of query and documents with averaging the vectors \n",
    "doc_representation = {}\n",
    "for query_id, term_id_list in query_dict.items():\n",
    "    word2vec_cosine_sim[query_id] = []\n",
    "    word2vec_kl_div[query_id] = []\n",
    "    q = [0.0] * 32 # 32 is the size of the embeddings\n",
    "    for term_id in term_id_list:\n",
    "        # ignore common words and word that appears less than 5 times in the whole collection\n",
    "        if term_id > 0 and id2token[term_id] in model :\n",
    "            q = np.add(q, model[id2token[term_id]])\n",
    "    q /= len(term_id_list)\n",
    "    for res in sorted_query_doc[query_id]:\n",
    "        doc_id = res[2]\n",
    "        d = [0.0] * 32 \n",
    "        # store doc representation in dictionary, don't have to calculate if it's already calculated before\n",
    "        if doc_id not in doc_representation:\n",
    "            doc = index.document(doc_id)\n",
    "            for word_id in doc[1]:\n",
    "                # ignore common words and word that appears less than 5 times in the whole collection\n",
    "                if word_id > 0 and id2token[word_id] in model:\n",
    "                    d = np.add(d, model[id2token[word_id]])\n",
    "            d /= len(sorted_query_doc[query_id])\n",
    "            doc_representation[doc_id] = d\n",
    "        word2vec_cosine_sim[query_id].append((cosine_similarity(q.reshape(1,-1),doc_representation[doc_id].reshape(1,-1))[0][0],index.document(doc_id)[0])) \n",
    "        word2vec_kl_div[query_id].append((mutual_info_score(q,doc_representation[doc_id]),index.document(doc_id)[0])) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Another Pre Processing (for LSI and LDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# corpus preparation for LSI and LDA model, reading all the documents in the collection\n",
    "dictionary = gensim.corpora.Dictionary(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# creating bag of words representation for each document in the collection (ordered sequentially)\n",
    "corpus = [dictionary.doc2bow(sentence) for sentence in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# creating bag of words representation for each query in the collection (ordered sequentially)\n",
    "query_corpus = []\n",
    "for query_id, term_id_list in query_dict.items():\n",
    "    term_word = [id2token[term_id] for term_id in term_id_list]\n",
    "    query_corpus.append(dictionary.doc2bow(term_word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# in case for storing file to disk\n",
    "import pickle\n",
    "def dump_file(obj,filename):\n",
    "    pickle.dump(obj, open(filename, 'wb'))\n",
    "    \n",
    "def read_file(filename):\n",
    "    return pickle.load(open('filename', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### LSI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# train LSI Model and create a 10 topic representation for document and a query\n",
    "# TAKES A LOT OF TIME\n",
    "lsi_model = gensim.models.LsiModel(corpus, id2word=dictionary, num_topics=10)\n",
    "doc_lsi_representation = lsi_model[corpus]\n",
    "query_lsi_representation = lsi_model[query_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create representation of doc_model and its doc_id\n",
    "doc_corpus_with_id = {}\n",
    "for doc_rep, document_id in zip(doc_lsi_representation, range(index.document_base(), index.maximum_document())):\n",
    "    doc_corpus_with_id[document_id] = doc_rep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert doc or query from tuple representation (topic_id, score) to only vector of scores \n",
    "def convert2vec(rep):\n",
    "    vec = [elem[1] for elem in rep]\n",
    "    return np.array(vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics.cluster import mutual_info_score \n",
    "lsi_cosine_sim = {}\n",
    "lsi_kl_div = {}\n",
    "# create representation of query and documents with averaging the vectors \n",
    "for query, q_rep in zip(query_dict.items(), query_lsi_representation):\n",
    "    lsi_cosine_sim[query[0]] = []\n",
    "    lsi_kl_div[query[0]] = []\n",
    "    q = convert2vec(q_rep)\n",
    "    for res in sorted_query_doc[query[0]]:\n",
    "        doc_id = res[2]\n",
    "        d_rep = doc_corpus_with_id[doc_id]\n",
    "        d = convert2vec(d_rep)\n",
    "        lsi_cosine_sim[query[0]].append((cosine_similarity(q.reshape(1,-1),d.reshape(1,-1))[0][0],index.document(doc_id)[0])) \n",
    "        lsi_kl_div[query[0]].append((mutual_info_score(q,d),index.document(doc_id)[0])) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train LSI Model and create a 10 topic representation for document and a query\n",
    "# TAKES A LOT OF TIME\n",
    "lda_model = gensim.models.LdaModel(corpus, id2word=dictionary, num_topics=10)\n",
    "doc_lda_representation = lda_model[corpus]\n",
    "query_lda_representation = lda_model[query_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create representation of doc_model and its doc_id\n",
    "doc_corpus_with_id_lda = {}\n",
    "for doc_rep, document_id in zip(doc_lda_representation, range(index.document_base(), index.maximum_document())):\n",
    "    doc_corpus_with_id_lda[document_id] = doc_rep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def convert2vec_lda(rep):\n",
    "    vec = [0.0] * 10\n",
    "    for elem in rep:\n",
    "        vec[elem[0]] = elem[1]\n",
    "    return np.array(vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 0.29394593628567878), (5, 0.35803121652693637), (6, 0.34141772007474525)]\n",
      "[(0, 0.88748875304359576), (1, 0.012500821494586945), (2, 0.012500500392128929), (3, 0.012501617173182152), (4, 0.012500382703892943), (5, 0.012504427668192927), (6, 0.012500567887173493), (7, 0.012500272183788122), (8, 0.012501192306339573), (9, 0.012501465147119164)]\n"
     ]
    }
   ],
   "source": [
    "print(doc_corpus_with_id_lda[52])\n",
    "print(query_lda_representation[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics.cluster import mutual_info_score \n",
    "lda_cosine_sim = {}\n",
    "lda_kl_div = {}\n",
    "# create representation of query and documents with averaging the vectors \n",
    "for query, q_rep in zip(query_dict.items(), query_lda_representation):\n",
    "    lda_cosine_sim[query[0]] = []\n",
    "    lda_kl_div[query[0]] = []\n",
    "    q = convert2vec_lda(q_rep)\n",
    "    for res in sorted_query_doc[query[0]]:\n",
    "        doc_id = res[2]\n",
    "        d_rep = doc_corpus_with_id_lda[doc_id]\n",
    "        d = convert2vec_lda(d_rep)\n",
    "        lda_cosine_sim[query[0]].append((cosine_similarity(q.reshape(1,-1),d.reshape(1,-1))[0][0],index.document(doc_id)[0])) \n",
    "        lda_kl_div[query[0]].append((mutual_info_score(q,d),index.document(doc_id)[0])) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "write_run(model_name='word2vec_cosine',\n",
    "          data=word2vec_cosine_sim,out_f=open('./ap_88_89/word2vec_cosine.run', 'w'),max_objects_per_query=1000)\n",
    "write_run(model_name='word2vec_kl_div',\n",
    "          data=word2vec_kl_div,out_f=open('./ap_88_89/word2vec_kl_div.run', 'w'),max_objects_per_query=1000)\n",
    "write_run(model_name='tf_idf',\n",
    "          data=tf_idf_scores,out_f=open('./ap_88_89/tf_idf_task_2.run', 'w'),max_objects_per_query=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "write_run(model_name='lsi_cosine',\n",
    "          data=lsi_cosine_sim,out_f=open('./ap_88_89/lsi_cosine.run', 'w'),max_objects_per_query=1000)\n",
    "write_run(model_name='lsi_kl_div',\n",
    "          data=lsi_kl_div,out_f=open('./ap_88_89/lsi_kl_div.run', 'w'),max_objects_per_query=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "write_run(model_name='lda_cosine',\n",
    "          data=lda_cosine_sim,out_f=open('./ap_88_89/lda_cosine.run', 'w'),max_objects_per_query=1000)\n",
    "write_run(model_name='lda_kl_div',\n",
    "          data=lda_kl_div,out_f=open('./ap_88_89/lda_kl_div.run', 'w'),max_objects_per_query=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To Do:\n",
    "- perform trec eval and significant testing\n",
    "- implement doc2vec\n",
    "- implement bag of words embeddings for the vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3: Learning to rank (LTR) [10 points] ###\n",
    "\n",
    "In this task you will get an introduction into learning to rank for information retrieval. You will experiment with a pointwise learning to rank method, logistic regression, implemented in [scikit-learn](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html).\n",
    "\n",
    "**NOTE**: you can only perform this task if you have completely finished Task 1 and Task 2.\n",
    "\n",
    "In this experiment, you will use the retrieval methods you implemented in Task 1 and Task 2 as features for the learning to rank model. Train your LTR model using 10-fold cross validation on the test set. For every query, first create a document candidate set using the top-1000 documents using TF-IDF. Secondly, compute query-document values using the retrieval models above and use them as features. Note that the feature values of different retrieval methods are likely to be distributed differently.\n",
    "\n",
    "Your approach will definitely not be as good as the state-of-the-art since you are taking a pointwise approach, but we do not ask you to try pair- or listwise methods because they will be the main topic of the next assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Task 4: Write a report [20 points; instant FAIL if not provided] ###\n",
    "\n",
    "The report should be a PDF file created using the [sigconf ACM template](https://www.acm.org/publications/proceedings-template) and will determine a significant part of your grade.\n",
    "\n",
    "   * It should explain what you have implemented, motivate your experiments and detail what you expect to learn from them. **[10 points]**\n",
    "   * Lastly, provide a convincing analysis of your results and conclude the report accordingly. **[10 points]**\n",
    "      * Do all methods perform similarly on all queries? Why?\n",
    "      * Is there a single retrieval model that outperforms all other retrieval models (i.e., silver bullet)?\n",
    "      * ...\n",
    "\n",
    "**Hand in the report and your self-contained implementation source files.** Do not send us the VirtualBox, but only the files that matter, organized in a well-documented zip/tgz file with clear instructions on how to reproduce your results. That is, we want to be able to regenerate all your results with minimal effort. You can assume that the index and ground-truth information is present in the same file system structure as on the VirtualBox.\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
